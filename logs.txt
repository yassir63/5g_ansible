ðŸ”¹ Running: ansible-playbook -i ./inventory/default/hosts.ini -e fiveg_profile=default  playbooks/deploy.yml
[WARNING]: Collection community.general does not support Ansible version
2.14.18
[WARNING]: Collection ansible.posix does not support Ansible version 2.14.18
[WARNING]: Collection kubernetes.core does not support Ansible version 2.14.18

PLAY [webshell] ****************************************************************

TASK [debug msg=Used inventory for run_pos playbook : {{ original_inventory }}] ***
ok: [localhost] => {}

MSG:

Used inventory for run_pos playbook : /home/yamami/5g_ansible_latest/inventory/default/hosts.ini

TASK [ansible.builtin.command chdir={{ playbook_dir | dirname }}, _raw_params=ansible-playbook -i {{ inventory_file }} playbooks/run_pos.yml --extra-vars "node={{ item }} no_boot={{ no_boot | default(false) }}" -c local
] ***
changed: [localhost] => (item=sopnode-f1)
changed: [localhost] => (item=sopnode-f2)
changed: [localhost] => (item=sopnode-w3)

TASK [async_status jid={{ item.ansible_job_id }}] ******************************
FAILED - RETRYING: [localhost]: async_status (100 retries left).
FAILED - RETRYING: [localhost]: async_status (99 retries left).
FAILED - RETRYING: [localhost]: async_status (98 retries left).
FAILED - RETRYING: [localhost]: async_status (97 retries left).
FAILED - RETRYING: [localhost]: async_status (96 retries left).
FAILED - RETRYING: [localhost]: async_status (95 retries left).
FAILED - RETRYING: [localhost]: async_status (94 retries left).
FAILED - RETRYING: [localhost]: async_status (93 retries left).
FAILED - RETRYING: [localhost]: async_status (92 retries left).
FAILED - RETRYING: [localhost]: async_status (91 retries left).
FAILED - RETRYING: [localhost]: async_status (90 retries left).
FAILED - RETRYING: [localhost]: async_status (89 retries left).
FAILED - RETRYING: [localhost]: async_status (88 retries left).
FAILED - RETRYING: [localhost]: async_status (87 retries left).
FAILED - RETRYING: [localhost]: async_status (86 retries left).
FAILED - RETRYING: [localhost]: async_status (85 retries left).
FAILED - RETRYING: [localhost]: async_status (84 retries left).
FAILED - RETRYING: [localhost]: async_status (83 retries left).
FAILED - RETRYING: [localhost]: async_status (82 retries left).
FAILED - RETRYING: [localhost]: async_status (81 retries left).
FAILED - RETRYING: [localhost]: async_status (80 retries left).
FAILED - RETRYING: [localhost]: async_status (79 retries left).
changed: [localhost] => (item=sopnode-f1)
changed: [localhost] => (item=sopnode-f2)
changed: [localhost] => (item=sopnode-w3)

PLAY [all] *********************************************************************

TASK [Gathering Facts ] ********************************************************
ok: [localhost]
ok: [sopnode-f2]
ok: [sopnode-f1]
ok: [sopnode-w3]

TASK [Define list of inventory booleans to normalize inventory_booleans=['fhi72', 'aw2s', 'f3_ran', 'bridge_enabled', 'monitoring_enabled']] ***
ok: [localhost]
ok: [sopnode-f2]
ok: [sopnode-w3]
ok: [sopnode-f1]

TASK [Force inventory booleans to real booleans {{ item }}={{ hostvars[inventory_hostname][item] | bool }}] ***
ok: [localhost] => (item=fhi72)
ok: [sopnode-f2] => (item=fhi72)
ok: [localhost] => (item=aw2s)
ok: [sopnode-w3] => (item=fhi72)
ok: [sopnode-f2] => (item=aw2s)
ok: [localhost] => (item=f3_ran)
ok: [sopnode-w3] => (item=aw2s)
ok: [sopnode-f2] => (item=f3_ran)
ok: [sopnode-f1] => (item=fhi72)
ok: [localhost] => (item=bridge_enabled)
ok: [sopnode-w3] => (item=f3_ran)
ok: [localhost] => (item=monitoring_enabled)
ok: [sopnode-f1] => (item=aw2s)
ok: [sopnode-f2] => (item=bridge_enabled)
ok: [sopnode-w3] => (item=bridge_enabled)
ok: [sopnode-f2] => (item=monitoring_enabled)
ok: [sopnode-f1] => (item=f3_ran)
ok: [sopnode-w3] => (item=monitoring_enabled)
ok: [sopnode-f1] => (item=bridge_enabled)
ok: [sopnode-f1] => (item=monitoring_enabled)

TASK [Debug normalized booleans msg={{ item }} = {{ hostvars[inventory_hostname][item] }}] ***
ok: [localhost] => (item=fhi72) => {}

MSG:

fhi72 = False
ok: [sopnode-f2] => (item=fhi72) => {}

MSG:

fhi72 = False
ok: [localhost] => (item=aw2s) => {}

MSG:

aw2s = False
ok: [sopnode-w3] => (item=fhi72) => {}

MSG:

fhi72 = False
ok: [localhost] => (item=f3_ran) => {}

MSG:

f3_ran = False
ok: [sopnode-f2] => (item=aw2s) => {}

MSG:

aw2s = False
ok: [sopnode-w3] => (item=aw2s) => {}

MSG:

aw2s = False
ok: [sopnode-f2] => (item=f3_ran) => {}

MSG:

f3_ran = False
ok: [localhost] => (item=bridge_enabled) => {}

MSG:

bridge_enabled = True
ok: [sopnode-f1] => (item=fhi72) => {}

MSG:

fhi72 = False
ok: [localhost] => (item=monitoring_enabled) => {}

MSG:

monitoring_enabled = True
ok: [sopnode-w3] => (item=f3_ran) => {}

MSG:

f3_ran = False
ok: [sopnode-f1] => (item=aw2s) => {}

MSG:

aw2s = False
ok: [sopnode-f2] => (item=bridge_enabled) => {}

MSG:

bridge_enabled = True
ok: [sopnode-w3] => (item=bridge_enabled) => {}

MSG:

bridge_enabled = True
ok: [sopnode-f2] => (item=monitoring_enabled) => {}

MSG:

monitoring_enabled = True
ok: [sopnode-f1] => (item=f3_ran) => {}

MSG:

f3_ran = False
ok: [sopnode-w3] => (item=monitoring_enabled) => {}

MSG:

monitoring_enabled = True
ok: [sopnode-f1] => (item=bridge_enabled) => {}

MSG:

bridge_enabled = True
ok: [sopnode-f1] => (item=monitoring_enabled) => {}

MSG:

monitoring_enabled = True

PLAY [Common sopnodes setup] ***************************************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/common : RESTART_AFTER_REBOOT _raw_params=noop] ********************

TASK [setup/common : RESTART_AFTER_REBOOT _raw_params=noop] ********************

TASK [setup/common : RESTART_AFTER_REBOOT _raw_params=noop] ********************

TASK [setup/common : Refresh boolean variables on all hosts fhi72={{ (hostvars[item].get('fhi72', False) | bool) }}, aw2s={{ (hostvars[item].get('aw2s', False) | bool) }}, f3_ran={{ (hostvars[item].get('f3_ran', False) | bool) }}, bridge_enabled={{ (hostvars[item].get('bridge_enabled', False) | bool) }}, monitoring_enabled={{ (hostvars[item].get('monitoring_enabled', False) | bool) }}] ***
ok: [sopnode-f1 -> localhost] => (item=localhost)
ok: [sopnode-f1 -> sopnode-f2] => (item=sopnode-f2)
ok: [sopnode-f1 -> sopnode-w3] => (item=sopnode-w3)
ok: [sopnode-f1] => (item=sopnode-f1)

TASK [setup/common : Ensure required apt packages are installed name=['emacs', 'tmux', 'git', 'curl', 'iproute2', 'iputils-ping', 'iperf3', 'tcpdump', 'python3-pip', 'python3-venv', 'jq'], state=present, update_cache=True] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Disable swap _raw_params=swapoff -a
sed -i.bak '/ swap / s/^/#/' /etc/fstab
] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Load kernel modules for Kubernetes networking name={{ item }}, state=present] ***
ok: [sopnode-f2] => (item=overlay)
ok: [sopnode-f1] => (item=overlay)
ok: [sopnode-w3] => (item=overlay)
changed: [sopnode-f1] => (item=br_netfilter)
changed: [sopnode-f2] => (item=br_netfilter)
changed: [sopnode-w3] => (item=br_netfilter)

TASK [setup/common : Ensure kernel modules are loaded on boot dest=/etc/modules-load.d/k8s.conf, content=overlay
br_netfilter
] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Set sysctl parameters for Kubernetes networking dest=/etc/sysctl.d/k8s.conf, content=net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Apply sysctl parameters _raw_params=sysctl --system] ******
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Set time zone to Europe/Paris _raw_params=timedatectl set-timezone Europe/Paris] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Verify time zone setting _raw_params=timedatectl] *********
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/common : Display timedatectl output msg={{ timedatectl_output.stdout_lines }}] ***
ok: [sopnode-f1] => {}

MSG:

['               Local time: Thu 2026-02-12 22:43:45 CET', '           Universal time: Thu 2026-02-12 21:43:45 UTC', '                 RTC time: Thu 2026-02-12 21:43:45', '                Time zone: Europe/Paris (CET, +0100)', 'System clock synchronized: yes', '              NTP service: active', '          RTC in local TZ: no']
ok: [sopnode-f2] => {}

MSG:

['               Local time: Thu 2026-02-12 22:43:45 CET', '           Universal time: Thu 2026-02-12 21:43:45 UTC', '                 RTC time: Thu 2026-02-12 21:43:45', '                Time zone: Europe/Paris (CET, +0100)', 'System clock synchronized: yes', '              NTP service: active', '          RTC in local TZ: no']
ok: [sopnode-w3] => {}

MSG:

['               Local time: Thu 2026-02-12 22:43:45 CET', '           Universal time: Thu 2026-02-12 21:43:45 UTC', '                 RTC time: Thu 2026-02-12 21:43:45', '                Time zone: Europe/Paris (CET, +0100)', 'System clock synchronized: yes', '              NTP service: active', '          RTC in local TZ: no']

TASK [setup/common : Install systemd service for CNI DHCP daemon dest=/etc/systemd/system/cni-dhcp.service, mode=0644, content=[Unit]
Description=CNI DHCP Daemon
After=network.target

[Service]
ExecStart=/opt/cni/bin/dhcp daemon
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/common : Reload systemd daemon_reload=True] ************************
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/common : Enable and start CNI DHCP daemon name=cni-dhcp.service, enabled=True, state=started] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/netplan : Upload netplan config to remote src=../{{ inventory_hostname }}.netplan.yaml, dest=/etc/netplan/50-radio.yaml, owner=root, group=root, mode=0644] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/netplan : Generate netplan config _raw_params=netplan generate] ****
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/netplan : Apply netplan config _raw_params=netplan apply] **********
changed: [sopnode-f2]
changed: [sopnode-f1]
changed: [sopnode-w3]

TASK [setup/netplan : Install lldpd name=lldpd, state=present] *****************
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/netplan : Enable and start lldpd service name=lldpd, enabled=True, state=started] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/containerd : RESTART_CONTAINERD _raw_params=noop] ******************

TASK [setup/containerd : RESTART_CONTAINERD _raw_params=noop] ******************

TASK [setup/containerd : RESTART_CONTAINERD _raw_params=noop] ******************

TASK [setup/containerd : Detect if running from a live Ubuntu image _raw_params=grep -q "boot=live" /proc/cmdline && echo "live" || echo "persistent"
] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/containerd : Create /var/lib/containerd directory path=/var/lib/containerd, state=directory, mode=0755] ***
skipping: [sopnode-f1]
skipping: [sopnode-w3]
changed: [sopnode-f2]

TASK [setup/containerd : Check if /var/lib/containerd is already mounted _raw_params=mountpoint -q /var/lib/containerd] ***
skipping: [sopnode-f1]
skipping: [sopnode-w3]
fatal: [sopnode-f2]: FAILED! => {
    "changed": false,
    "cmd": [
        "mountpoint",
        "-q",
        "/var/lib/containerd"
    ],
    "delta": "0:00:00.002082",
    "end": "2026-02-12 22:44:05.382588",
    "rc": 32,
    "start": "2026-02-12 22:44:05.380506"
}

MSG:

non-zero return code
...ignoring

TASK [setup/containerd : Mount tmpfs on /var/lib/containerd only if not already mounted path=/var/lib/containerd, src=tmpfs, fstype=tmpfs, opts=size=10G, state=mounted] ***
skipping: [sopnode-f1]
skipping: [sopnode-w3]
changed: [sopnode-f2]

TASK [setup/containerd : Remove old Docker repo junk path={{ item }}, state=absent] ***
changed: [sopnode-f2] => (item=/etc/apt/sources.list.d/docker.list)
changed: [sopnode-f1] => (item=/etc/apt/sources.list.d/docker.list)
changed: [sopnode-w3] => (item=/etc/apt/sources.list.d/docker.list)
changed: [sopnode-f2] => (item=/etc/apt/keyrings/docker.asc)
changed: [sopnode-f1] => (item=/etc/apt/keyrings/docker.asc)
changed: [sopnode-w3] => (item=/etc/apt/keyrings/docker.asc)

TASK [setup/containerd : Install containerd name=containerd, state=present, update_cache=True] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/containerd : Ensure /etc/containerd exists path=/etc/containerd, state=directory, mode=0755] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/containerd : Generate default containerd config _raw_params=containerd config default] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/containerd : Write config.toml dest=/etc/containerd/config.toml, content={{ containerd_cfg.stdout }}, owner=root, group=root, mode=0644] ***
changed: [sopnode-f1]
changed: [sopnode-w3]
changed: [sopnode-f2]

TASK [setup/containerd : Enable systemd-cgroup (compat for old/new containerd) path=/etc/containerd/config.toml, regexp=(SystemdCgroup = false|systemd_cgroup = false), replace=SystemdCgroup = true] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/containerd : Ensure plugin section also enables systemd cgroup path=/etc/containerd/config.toml, insertafter=^\[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options\], line=            SystemdCgroup = true, state=present] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/containerd : Switch snapshotter to native path=/etc/containerd/config.toml, regexp=snapshotter = "overlayfs", replace=snapshotter = "native"] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/containerd : Force correct pause image for Kubernetes >= 1.32 path=/etc/containerd/config.toml, regexp=sandbox_image\s*=\s*".*", replace=sandbox_image = "registry.k8s.io/pause:3.10"] ***
changed: [sopnode-f1]
changed: [sopnode-w3]
changed: [sopnode-f2]

TASK [setup/containerd : Restart & enable containerd name=containerd, enabled=True, state=restarted, daemon_reload=True] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/containerd : Wait for containerd socket path=/run/containerd/containerd.sock, state=present, timeout=20] ***
ok: [sopnode-f2]
ok: [sopnode-f1]
ok: [sopnode-w3]

TASK [setup/containerd : Display containerd cgroup driver for debug _raw_params=grep -E 'SystemdCgroup|systemd_cgroup' /etc/containerd/config.toml | head -n 3
] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/containerd : ansible.builtin.debug var=cgroup_check.stdout] ********
ok: [sopnode-f1] => {
    "cgroup_check.stdout": "    SystemdCgroup = true\n            SystemdCgroup = true"
}
ok: [sopnode-f2] => {
    "cgroup_check.stdout": "    SystemdCgroup = true\n            SystemdCgroup = true"
}
ok: [sopnode-w3] => {
    "cgroup_check.stdout": "    SystemdCgroup = true\n            SystemdCgroup = true"
}

TASK [setup/pre_k8s : Detect live Ubuntu system _raw_params=grep -q "boot=live" /proc/cmdline && echo "live" || echo "persistent"
] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Create containerd directory path=/var/lib/containerd, state=directory, mode=0755] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Mount tmpfs on /var/lib/containerd (50G) path=/var/lib/containerd, src=tmpfs, fstype=tmpfs, opts=size=50G, state=mounted] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Ensure directories exist path={{ item }}, state=directory, mode=0755] ***
skipping: [sopnode-f1] => (item=/var/lib/containerd) 
skipping: [sopnode-f1] => (item=/var/lib/kubelet) 
skipping: [sopnode-f1]
skipping: [sopnode-f2] => (item=/var/lib/containerd) 
skipping: [sopnode-f2] => (item=/var/lib/kubelet) 
skipping: [sopnode-f2]
skipping: [sopnode-w3] => (item=/var/lib/containerd) 
skipping: [sopnode-w3] => (item=/var/lib/kubelet) 
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Mount tmpfs on /var/lib/containerd (50G) path=/var/lib/containerd, src=tmpfs, fstype=tmpfs, opts=size=50G, state=mounted] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Mount tmpfs on /var/lib/kubelet (5G) path=/var/lib/kubelet, src=tmpfs, fstype=tmpfs, opts=size=5G, state=mounted] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Ensure storage device exists path=/dev/{{ storage }}] ****
ok: [sopnode-f2]
ok: [sopnode-f1]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Fail if storage device does not exist msg=Storage device /dev/{{ storage }} does not exist] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Create containerd mount point path=/var/lib/containerd, state=directory, mode=0755] ***
changed: [sopnode-f1]
ok: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Mount persistent storage for containerd path=/var/lib/containerd, src=/dev/{{ storage }}, fstype=ext4, state=mounted] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Ensure kubelet base directory exists path=/var/lib/kubelet, state=directory, mode=0755] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Check if kubelet config exists path=/var/lib/kubelet/config.yaml] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Backup existing kubelet config if present src=/var/lib/kubelet/config.yaml, dest=/var/lib/kubelet/config.yaml.bak.{{ ansible_date_time.epoch }}, remote_src=True] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Ensure kubelet config exists path=/var/lib/kubelet/config.yaml, state=touch, mode=0644] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Patch kubelet config (Unified) path=/var/lib/kubelet/config.yaml, block=rootDir: /var/lib/containerd
evictionHard:
  nodefs.available: "{{ '2%' if (core_node_name == ran_node_name) else '10%' }}"
  imagefs.available: "{{ '2%' if (core_node_name == ran_node_name) else '10%' }}"
{% if core_node_name == ran_node_name %}
evictionMinimumReclaim:
  nodefs.available: "500Mi"
{% endif %}
, marker=# {mark} KUBELET STORAGE PATCH] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Check if containerd service exists _raw_params=systemctl list-unit-files | grep -q '^containerd\.service'] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Reload systemd and restart containerd name=containerd, state=restarted, enabled=True, daemon_reload=True] ***
changed: [sopnode-f2]
changed: [sopnode-f1]
changed: [sopnode-w3]

TASK [setup/pre_k8s : Check if kubelet service exists _raw_params=systemctl list-unit-files | grep -q '^kubelet\.service'] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Restart kubelet name=kubelet, state=restarted, enabled=True, daemon_reload=True] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/pre_k8s : Check /var/lib/containerd mount _raw_params=mount | grep /var/lib/containerd || true] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Show /var/lib/containerd mount status msg={{ containerd_mount.stdout if containerd_mount.stdout else 'not mounted' }}] ***
ok: [sopnode-f1] => {}

MSG:

tmpfs on /var/lib/containerd type tmpfs (rw,relatime,size=52428800k,inode64)
ok: [sopnode-f2] => {}

MSG:

tmpfs on /var/lib/containerd type tmpfs (rw,relatime,size=52428800k,inode64)
ok: [sopnode-w3] => {}

MSG:

tmpfs on /var/lib/containerd type tmpfs (rw,relatime,size=52428800k,inode64)

TASK [setup/pre_k8s : Verify kubelet rootDir and evictionHard _raw_params=grep -A 3 'evictionHard' /var/lib/kubelet/config.yaml || true; grep 'rootDir' /var/lib/kubelet/config.yaml || true] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/pre_k8s : Show kubelet config check msg={{ kubelet_config_check.stdout }}] ***
ok: [sopnode-f1] => {}

MSG:

evictionHard:
  nodefs.available: "10%"
  imagefs.available: "10%"
# END KUBELET STORAGE PATCH
rootDir: /var/lib/containerd
ok: [sopnode-f2] => {}

MSG:

evictionHard:
  nodefs.available: "10%"
  imagefs.available: "10%"
# END KUBELET STORAGE PATCH
rootDir: /var/lib/containerd
ok: [sopnode-w3] => {}

MSG:

evictionHard:
  nodefs.available: "10%"
  imagefs.available: "10%"
# END KUBELET STORAGE PATCH
rootDir: /var/lib/containerd

TASK [setup/pre_k8s : Fail if disk pressure patch not applied msg=Disk pressure fix not applied: check /var/lib/containerd mount or kubelet config] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Create /etc/apt/keyrings directory path=/etc/apt/keyrings, state=directory, mode=0755] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Download Kubernetes GPG key creates=/etc/apt/keyrings/kubernetes-apt-keyring.gpg, _raw_params=curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key |  gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg
] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Add Kubernetes APT repository (v1.32) repo=deb [arch={{ 'amd64' if ansible_architecture == 'x86_64' else 'arm64' }} signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /, state=present, filename=kubernetes, update_cache=True] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Update APT cache update_cache=True] ****************
ok: [sopnode-f2]
ok: [sopnode-f1]
ok: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Install kubelet, kubeadm, kubectl (exact version 1.32.9) name=['kubelet=1.32.9-1.1', 'kubeadm=1.32.9-1.1', 'kubectl=1.32.9-1.1'], state=present, update_cache=False] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Verify kubeadm is installed _raw_params=kubeadm version] ***
ok: [sopnode-f2]
ok: [sopnode-f1]
ok: [sopnode-w3]

TASK [setup/k8s/k8s_setup : Hold Kubernetes packages name={{ item }}, selection=hold] ***
changed: [sopnode-f2] => (item=kubelet)
changed: [sopnode-f1] => (item=kubelet)
changed: [sopnode-w3] => (item=kubelet)
changed: [sopnode-f1] => (item=kubeadm)
changed: [sopnode-f2] => (item=kubeadm)
changed: [sopnode-w3] => (item=kubeadm)
changed: [sopnode-f1] => (item=kubectl)
changed: [sopnode-f2] => (item=kubectl)
changed: [sopnode-w3] => (item=kubectl)

TASK [setup/k8s/k8s_env : RESTART_K8S_ENV _raw_params=noop] ********************

TASK [setup/k8s/k8s_env : RESTART_K8S_ENV _raw_params=noop] ********************

TASK [setup/k8s/k8s_env : RESTART_K8S_ENV _raw_params=noop] ********************

TASK [setup/k8s/k8s_env : Download k9s .deb package using wget creates={{ k9s_deb_path }}, _raw_params=wget -O {{ k9s_deb_path }} https://github.com/derailed/k9s/releases/latest/download/k9s_linux_amd64.deb
] ***
changed: [sopnode-w3]
changed: [sopnode-f2]
changed: [sopnode-f1]

TASK [setup/k8s/k8s_env : Install k9s deb={{ k9s_deb_path }}, state=present] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/k8s_env : Ensure k9s binary is available _raw_params=k9s version] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/k8s_env : Display k9s version var=k9s_check.stdout] ************
ok: [sopnode-f1] => {
    "k9s_check.stdout": "\u001b[36m ____  __ ________       \u001b[0m\n\u001b[36m|    |/  /   __   \\______\u001b[0m\n\u001b[36m|       /\\____    /  ___/\u001b[0m\n\u001b[36m|    \\   \\  /    /\\___  \\\u001b[0m\n\u001b[36m|____|\\__ \\/____//____  /\u001b[0m\n\u001b[36m         \\/           \\/ \u001b[0m\n\n\u001b[36mVersion:\u001b[0m    v0.50.18\n\u001b[36mCommit:\u001b[0m     6dbf571c59fd48dc5b384aa46ee7f3e5decfae2b\n\u001b[36mDate:\u001b[0m       2026-01-11T20:09:14Z"
}
ok: [sopnode-f2] => {
    "k9s_check.stdout": "\u001b[36m ____  __ ________       \u001b[0m\n\u001b[36m|    |/  /   __   \\______\u001b[0m\n\u001b[36m|       /\\____    /  ___/\u001b[0m\n\u001b[36m|    \\   \\  /    /\\___  \\\u001b[0m\n\u001b[36m|____|\\__ \\/____//____  /\u001b[0m\n\u001b[36m         \\/           \\/ \u001b[0m\n\n\u001b[36mVersion:\u001b[0m    v0.50.18\n\u001b[36mCommit:\u001b[0m     6dbf571c59fd48dc5b384aa46ee7f3e5decfae2b\n\u001b[36mDate:\u001b[0m       2026-01-11T20:09:14Z"
}
ok: [sopnode-w3] => {
    "k9s_check.stdout": "\u001b[36m ____  __ ________       \u001b[0m\n\u001b[36m|    |/  /   __   \\______\u001b[0m\n\u001b[36m|       /\\____    /  ___/\u001b[0m\n\u001b[36m|    \\   \\  /    /\\___  \\\u001b[0m\n\u001b[36m|____|\\__ \\/____//____  /\u001b[0m\n\u001b[36m         \\/           \\/ \u001b[0m\n\n\u001b[36mVersion:\u001b[0m    v0.50.18\n\u001b[36mCommit:\u001b[0m     6dbf571c59fd48dc5b384aa46ee7f3e5decfae2b\n\u001b[36mDate:\u001b[0m       2026-01-11T20:09:14Z"
}

TASK [setup/k8s/k8s_env : Ensure kubectl alias k exists for root path=/root/.bashrc, regexp=^alias k=, line=alias k=kubectl, state=present] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/k8s_env : Ensure kubectl namespace alias kns exists for root path=/root/.bashrc, regexp=^kns\(\), line=kns() { kubectl config set-context --current --namespace="$1"; }, state=present] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/k8s_env : Install custom script to /usr/local/bin src={{ playbook_dir }}/../debug-scripts/logs.sh, dest=/usr/local/bin/logs.sh, mode=0755] ***
[WARNING]: noop task does not support when conditional
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/ovs : RESTART_OVS_SETUP _raw_params=noop] **************************

TASK [setup/ovs : RESTART_OVS_SETUP _raw_params=noop] **************************

TASK [setup/ovs : RESTART_OVS_SETUP _raw_params=noop] **************************

TASK [setup/ovs : Ensure OVS is installed name=openvswitch-switch, state=present, update_cache=True] ***
changed: [sopnode-f2]
changed: [sopnode-f1]
changed: [sopnode-w3]

TASK [setup/ovs : Ensure OVS services are running name={{ item }}, state=started, enabled=True] ***
ok: [sopnode-f2] => (item=ovsdb-server)
ok: [sopnode-f1] => (item=ovsdb-server)
ok: [sopnode-w3] => (item=ovsdb-server)
ok: [sopnode-f2] => (item=ovs-vswitchd)
ok: [sopnode-f1] => (item=ovs-vswitchd)
ok: [sopnode-w3] => (item=ovs-vswitchd)

TASK [setup/ovs : Define OVS bridges ovs_bridges=['n2br', 'n3br', 'n4br']] *****
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/ovs : Create OVS bridges _raw_params=ovs-vsctl --may-exist add-br {{ item }}] ***
changed: [sopnode-f1] => (item=n2br)
changed: [sopnode-f2] => (item=n2br)
changed: [sopnode-w3] => (item=n2br)
changed: [sopnode-f1] => (item=n3br)
changed: [sopnode-f2] => (item=n3br)
changed: [sopnode-w3] => (item=n3br)
changed: [sopnode-f1] => (item=n4br)
changed: [sopnode-f2] => (item=n4br)
changed: [sopnode-w3] => (item=n4br)

TASK [setup/ovs : Enable CPUAffinity override dir path=/etc/systemd/system/ovs-vswitchd.service.d, state=directory] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/ovs : Apply CPUAffinity for ovs-vswitchd dest=/etc/systemd/system/ovs-vswitchd.service.d/cpu-affinity.conf, content=[Service]
CPUAffinity=0-5
] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/ovs : Restart ovs-vswitchd with new affinity daemon_reload=True, name=ovs-vswitchd, state=restarted] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/ovs : Verify OVS daemons ] *****************************************
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/ovs : Fail if ovsdb-server is not running msg=âŒ ovsdb-server is not running on {{ inventory_hostname }}] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/ovs : Fail if ovs-vswitchd is not running msg=âŒ ovs-vswitchd is not running on {{ inventory_hostname }}] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/ovs : Verify OVS bridges _raw_params=ovs-vsctl br-exists {{ item }}] ***
ok: [sopnode-f1] => (item=n2br)
ok: [sopnode-f2] => (item=n2br)
ok: [sopnode-w3] => (item=n2br)
ok: [sopnode-f1] => (item=n3br)
ok: [sopnode-f2] => (item=n3br)
ok: [sopnode-w3] => (item=n3br)
ok: [sopnode-f1] => (item=n4br)
ok: [sopnode-f2] => (item=n4br)
ok: [sopnode-w3] => (item=n4br)

TASK [setup/optimization/cpu : CPU_CHECK _raw_params=noop] *********************

TASK [setup/optimization/cpu : CPU_CHECK _raw_params=noop] *********************

TASK [setup/optimization/cpu : CPU_CHECK _raw_params=noop] *********************

TASK [setup/optimization/cpu : Install cpupower utility name=['linux-tools-common', 'linux-tools-{{ ansible_kernel }}', 'cpufrequtils'], state=present] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/optimization/cpu : Install tuned if needed name=tuned, state=present, update_cache=True] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/optimization/cpu : Ensure tuned service is running and enabled name=tuned, state=started, enabled=True] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/optimization/cpu : Get current tuned profile _raw_params=tuned-adm active] ***
ok: [sopnode-f1]
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/optimization/cpu : Set tuned profile to throughput-performance _raw_params=tuned-adm profile throughput-performance] ***
changed: [sopnode-f1]
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/optimization/cpu : Apply sysctl tuning parameters name={{ item.key }}, value={{ item.val }}, sysctl_set=True, state=present, reload=True] ***
changed: [sopnode-f1] => (item={'key': 'net.core.rmem_default', 'val': '134217728'})
changed: [sopnode-f2] => (item={'key': 'net.core.rmem_default', 'val': '134217728'})
changed: [sopnode-w3] => (item={'key': 'net.core.rmem_default', 'val': '134217728'})
changed: [sopnode-f1] => (item={'key': 'net.core.rmem_max', 'val': '134217728'})
changed: [sopnode-f2] => (item={'key': 'net.core.rmem_max', 'val': '134217728'})
changed: [sopnode-f1] => (item={'key': 'net.core.wmem_default', 'val': '134217728'})
changed: [sopnode-f2] => (item={'key': 'net.core.wmem_default', 'val': '134217728'})
changed: [sopnode-w3] => (item={'key': 'net.core.rmem_max', 'val': '134217728'})
changed: [sopnode-f1] => (item={'key': 'net.core.wmem_max', 'val': '134217728'})
changed: [sopnode-f2] => (item={'key': 'net.core.wmem_max', 'val': '134217728'})
changed: [sopnode-f1] => (item={'key': 'net.core.netdev_max_backlog', 'val': '5000'})
changed: [sopnode-f2] => (item={'key': 'net.core.netdev_max_backlog', 'val': '5000'})
changed: [sopnode-f2] => (item={'key': 'net.core.optmem_max', 'val': '524288'})
changed: [sopnode-f1] => (item={'key': 'net.core.optmem_max', 'val': '524288'})
changed: [sopnode-w3] => (item={'key': 'net.core.wmem_default', 'val': '134217728'})
changed: [sopnode-f1] => (item={'key': 'kernel.sched_rt_runtime_us', 'val': '-1'})
changed: [sopnode-f2] => (item={'key': 'kernel.sched_rt_runtime_us', 'val': '-1'})
changed: [sopnode-w3] => (item={'key': 'net.core.wmem_max', 'val': '134217728'})
changed: [sopnode-f1] => (item={'key': 'kernel.timer_migration', 'val': '0'})
changed: [sopnode-f2] => (item={'key': 'kernel.timer_migration', 'val': '0'})
changed: [sopnode-w3] => (item={'key': 'net.core.netdev_max_backlog', 'val': '5000'})
changed: [sopnode-w3] => (item={'key': 'net.core.optmem_max', 'val': '524288'})
changed: [sopnode-w3] => (item={'key': 'kernel.sched_rt_runtime_us', 'val': '-1'})
changed: [sopnode-w3] => (item={'key': 'kernel.timer_migration', 'val': '0'})

TASK [setup/optimization/cpu : Create /etc/sysctl.d/rt.conf dest=/etc/sysctl.d/rt.conf, content=kernel.sched_rt_runtime_us=-1
kernel.timer_migration=0
, owner=root, group=root, mode=0644] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Apply sysctl settings from /etc/sysctl.d/rt.conf _raw_params=sysctl -p /etc/sysctl.d/rt.conf] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Check if hyperthreading is disabled (reliable method) _raw_params=threads_per_core=$(lscpu | awk '/^Thread\(s\) per core:/ {print $4}')
if [ "$threads_per_core" -eq 1 ]; then
  echo "Hyperthreading is disabled"
  exit 0
else
  echo "Hyperthreading is enabled"
  exit 1
fi
] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Show hyperthreading status msg={{ ht_check.stdout }}] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Get CPU idle states _raw_params=cpupower idle-info] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Disable all CPU idle states _raw_params=for state in $(cpupower idle-info | awk '/Available idle states/ {for(i=2;i<=NF;i++) print $i}'); do
  cpupower idle-set -d $state || true
done
] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Show CPU idle states after disabling _raw_params=cpupower idle-info] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/optimization/cpu : Display CPU idle states msg={{ idle_info_after.stdout_lines }}] ***
skipping: [sopnode-f1]
skipping: [sopnode-f2]
skipping: [sopnode-w3]

PLAY [PTP setup on ran_node] ***************************************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f2]

TASK [setup/optimization/nic : Install ethtool name=['ethtool'], state=present, update_cache=True] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Increase NIC ring buffers on ens2f1 _raw_params=ethtool -G {{ nic_interface }} tx 8160 rx 8160] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Install tuned name=['tuned'], state=present, update_cache=True] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Activate throughput-performance tuned profile _raw_params=tuned-adm profile throughput-performance] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Show active tuned profile _raw_params=tuned-adm active] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : debug msg={{ tuned_status.stdout }}] ************
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Find IRQs for ens15f0np0 _raw_params=grep -i 'ens15f0np0' /proc/interrupts | awk '{print $1}' | tr -d ':'] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Apply IRQ affinity live (use CPU list instead of masks) _raw_params=echo 14-31 > /proc/irq/{{ item }}/smp_affinity_list
] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Make IRQ pinning persistent on reboot dest=/etc/systemd/system/irq-affinity-ens15f0np0.service, content=[Unit]
Description=Pin IRQs for ens15f0np0
After=multi-user.target

[Service]
Type=oneshot
ExecStart=/bin/sh -c 'for i in {{ ovs_irq_list.stdout_lines | join(" ") }}; do echo 14-31 > /proc/irq/$i/smp_affinity_list; done'

[Install]
WantedBy=multi-user.target
] ***
skipping: [sopnode-f2]

TASK [setup/optimization/nic : Enable IRQ affinity service daemon_reload=True, name=irq-affinity-ens15f0np0.service, enabled=True] ***
skipping: [sopnode-f2]

TASK [setup/ptp : RESTART_PTP _raw_params=noop] ********************************

TASK [setup/ptp : Install build dependencies for linuxptp v4.4 name=['gcc', 'make', 'libpcap-dev', 'linux-headers-{{ ansible_kernel }}', 'git'], state=present, update_cache=True] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Clone latest linuxptp repo=https://github.com/richardcochran/linuxptp.git, dest=/tmp/linuxptp, version=v4.4, force=True, depth=1] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Build and install linuxptp v4.4 to /usr/local/sbin creates=/usr/local/sbin/ptp4l, _raw_params=cd /tmp/linuxptp
make -j$(nproc)
make install 
] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Copy ptp4l and phc2sys to /usr/sbin src={{ item.src }}, dest={{ item.dest }}, mode=0755, remote_src=True] ***
skipping: [sopnode-f2] => (item={'src': '/usr/local/sbin/ptp4l', 'dest': '/usr/sbin/ptp4l'}) 
skipping: [sopnode-f2] => (item={'src': '/usr/local/sbin/phc2sys', 'dest': '/usr/sbin/phc2sys'}) 
skipping: [sopnode-f2]

TASK [setup/ptp : Ensure /etc/sysconfig exists path=/etc/sysconfig, state=directory, mode=0755] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Install ptp4l.conf src=ptp4l.conf.j2, dest=/etc/ptp4l.conf, mode=0644] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Install /etc/sysconfig/ptp4l src=sysconfig_ptp4l.j2, dest=/etc/sysconfig/ptp4l, mode=0644] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Install /etc/sysconfig/phc2sys src=sysconfig_phc2sys.j2, dest=/etc/sysconfig/phc2sys, mode=0644] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Install ptp4l systemd unit src=ptp4l.service.j2, dest=/usr/lib/systemd/system/ptp4l.service, mode=0644] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Install phc2sys systemd unit src=phc2sys.service.j2, dest=/usr/lib/systemd/system/phc2sys.service, mode=0644] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Disable NTP via timedatectl _raw_params=timedatectl set-ntp false] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Set timezone to Europe/Paris name=Europe/Paris] **************
skipping: [sopnode-f2]

TASK [setup/ptp : Reload systemd daemon daemon_reload=True] ********************
skipping: [sopnode-f2]

TASK [setup/ptp : Enable and start ptp4l name=ptp4l, enabled=True, state=started] ***
skipping: [sopnode-f2]

TASK [setup/ptp : Enable and start phc2sys name=phc2sys, enabled=True, state=started] ***
skipping: [sopnode-f2]

PLAY [Create and setup Kubernetes Cluster on Master] ***************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f1]

TASK [setup/ntp : Disable NTP via timedatectl _raw_params=timedatectl set-ntp false] ***
skipping: [sopnode-f1]

TASK [setup/ntp : Set timezone to Europe/Paris name=Europe/Paris] **************
skipping: [sopnode-f1]

TASK [setup/ntp : Fetch current date from sopnode-f2 _raw_params=ssh {{ groups['ran_node'][0] }} date --iso-8601=seconds] ***
skipping: [sopnode-f1]

TASK [setup/ntp : Set sopnode-f1 date to match sopnode-f2 _raw_params=ssh {{ groups['core_node'][0] }} sudo date --set "{{ f3_date.stdout }}"] ***
skipping: [sopnode-f1]

TASK [setup/k8s/cluster_create : RESTART_K8S_CREATE _raw_params=noop] **********

TASK [setup/k8s/cluster_create : Normalize booleans for current host fhi72={{ fhi72 | default(false) | bool }}, bridge_enabled={{ bridge_enabled | default(false) | bool }}, monitoring_enabled={{ monitoring_enabled | default(false) | bool }}, f3_ran={{ f3_ran | default(false) | bool }}] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Reset Kubernetes (safe) _raw_params=kubeadm reset -f] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Remove old Kubernetes and CNI directories (safe) path={{ item }}, state=absent] ***
changed: [sopnode-f1] => (item=/etc/kubernetes)
ok: [sopnode-f1] => (item=/var/lib/etcd)
changed: [sopnode-f1] => (item=/var/lib/kubelet)
changed: [sopnode-f1] => (item=/etc/cni)
changed: [sopnode-f1] => (item=/opt/cni)
ok: [sopnode-f1] => (item=/var/lib/cni)
ok: [sopnode-f1] => (item=/run/flannel)

TASK [setup/k8s/cluster_create : Ensure /var/lib/containerd is mounted on tmpfs (>=50G) _raw_params=current_mount=$(mountpoint -q /var/lib/containerd && df -h --output=fstype,size /var/lib/containerd | tail -n1 || echo "none 0G")
fstype=$(echo "$current_mount" | awk '{print $1}')
size=$(echo "$current_mount" | awk '{print $2}' | sed 's/G//')
if [ "$fstype" != "tmpfs" ] || [ "${size:-0}" -lt 50 ]; then
  echo "Mounting tmpfs on /var/lib/containerd (50G)"
  umount -f /var/lib/containerd 2>/dev/null || true
  mount -t tmpfs -o size=50G tmpfs /var/lib/containerd
else
  echo "tmpfs already mounted on /var/lib/containerd with ${size}G"
fi
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Restart containerd after remount name=containerd, state=restarted] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Wait 5 seconds for containerd to settle seconds=5] ***
Pausing for 5 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Upload kubeadm config to master node src=kubeadm-config.yaml.j2, dest=/root/kubeadm-config.yaml] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Initialize Kubernetes cluster _raw_params=kubeadm init --config /root/kubeadm-config.yaml] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Wait until at least one kube-proxy pod exists _raw_params=until kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system get pod -l k8s-app=kube-proxy --no-headers 2>/dev/null | grep .; do
  echo "Waiting for kube-proxy pod to appear..."
  sleep 5
done
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Restart kube-proxy pods to reload config.conf (if any exist) _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system delete pod -l k8s-app=kube-proxy --ignore-not-found
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Wait until kube-proxy pods are Running _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system wait pod -l k8s-app=kube-proxy --for=condition=Ready --timeout=180s || true
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Check for deprecated kubelet flags in /etc/default/kubelet _raw_params=if [ -f /etc/default/kubelet ]; then
  if grep -Eq -- '--network-plugin|--cni-bin-dir|--cni-conf-dir' /etc/default/kubelet; then
    echo "DEPRECATED_FOUND"
  else
    echo "CLEAN"
  fi
else
  echo "MISSING"
fi
] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Backup existing /etc/default/kubelet if deprecated flags found src=/etc/default/kubelet, dest=/etc/default/kubelet.bak_{{ ansible_date_time.epoch }}, remote_src=True] ***
skipping: [sopnode-f1]

TASK [setup/k8s/cluster_create : Sanitize /etc/default/kubelet (write only the resolvConf) if deprecated flags found or file missing dest=/etc/default/kubelet, content=KUBELET_EXTRA_ARGS="--resolv-conf=/run/systemd/resolve/resolv.conf", mode=0644] ***
skipping: [sopnode-f1]

TASK [setup/k8s/cluster_create : Ensure kubelet systemd sees the change (daemon-reload) daemon_reload=True] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Restart kubelet if we changed the file name=kubelet, state=restarted] ***
skipping: [sopnode-f1]

TASK [setup/k8s/cluster_create : Ensure kubelet uses systemd-resolved DNS dest=/etc/default/kubelet, content=KUBELET_EXTRA_ARGS=--resolv-conf=/run/systemd/resolve/resolv.conf
, mode=0644] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Restart kubelet to apply DNS config name=kubelet, state=restarted] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Enable cluster CIDR allocation in kube-controller-manager path=/etc/kubernetes/manifests/kube-controller-manager.yaml, regexp={{ item.regexp }}, line={{ item.line }}] ***
ok: [sopnode-f1] => (item={'regexp': '^\\s*- --allocate-node-cidrs=', 'line': '    - --allocate-node-cidrs=true'})
ok: [sopnode-f1] => (item={'regexp': '^\\s*- --cluster-cidr=', 'line': '    - --cluster-cidr=10.244.0.0/16'})

TASK [setup/k8s/cluster_create : Create .kube directory in user's home path={{ ansible_env.HOME }}/.kube, state=directory, mode=0700] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Copy admin.conf to kube config src=/etc/kubernetes/admin.conf, dest={{ ansible_env.HOME }}/.kube/config, remote_src=True, mode=0600] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Ensure Helm and Python tooling are available name=['python3-pip', 'curl'], state=present, update_cache=True] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Install kubernetes Python module name=kubernetes] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Install Helm if missing creates=/usr/local/bin/helm, _raw_params=curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Ensure CNI directories exist path={{ item }}, state=directory, mode=0755] ***
changed: [sopnode-f1] => (item=/etc/cni/net.d)
changed: [sopnode-f1] => (item=/opt/cni/bin)
changed: [sopnode-f1] => (item=/var/lib/cni)

TASK [setup/k8s/cluster_create : Clean old or conflicting CNI configs path={{ item }}, state=absent] ***
ok: [sopnode-f1] => (item=/etc/cni/net.d/00-multus.conf)
ok: [sopnode-f1] => (item=/etc/cni/net.d/00-multus.conf.bak)
ok: [sopnode-f1] => (item=/etc/cni/net.d/10-flannel.conf)
ok: [sopnode-f1] => (item=/etc/cni/net.d/10-flannel.conf~)

TASK [setup/k8s/cluster_create : Deploy Flannel CNI conflist dest=/etc/cni/net.d/10-flannel.conflist, mode=0644, content={
  "name": "cbr0",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Download CNI plugins tarball url=https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz, dest=/tmp/cni-plugins.tgz, mode=0644] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Extract CNI plugins src=/tmp/cni-plugins.tgz, dest=/opt/cni/bin, remote_src=True] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Ensure kube-flannel namespace exists definition={'apiVersion': 'v1', 'kind': 'Namespace', 'metadata': {'name': 'kube-flannel', 'labels': {'pod-security.kubernetes.io/enforce': 'privileged'}}}, state=present, kubeconfig=/etc/kubernetes/admin.conf] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Add Flannel Helm repo name=flannel, repo_url=https://flannel-io.github.io/flannel/, kubeconfig=/etc/kubernetes/admin.conf] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Install Flannel via Helm name=flannel, chart_ref=flannel/flannel, release_namespace=kube-flannel, state=present, wait=True, kubeconfig=/etc/kubernetes/admin.conf, values={'podCidr': '10.244.0.0/16', 'flannel': {'iface': "{{ hostvars[inventory_hostname].nic_interface | default('eth0') }}"}}] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Restart containerd and kubelet to reload CNI name={{ item }}, state=restarted] ***
changed: [sopnode-f1] => (item=containerd)
changed: [sopnode-f1] => (item=kubelet)

TASK [setup/k8s/cluster_create : Wait until node is Ready _raw_params=kubectl wait node $(hostname) --for=condition=Ready --timeout=180s
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Wait for CoreDNS pods to be Ready _raw_params=kubectl wait -n kube-system --for=condition=Ready pod -l k8s-app=kube-dns --timeout=300s
] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Test CoreDNS resolution using a non-interactive busybox pod _raw_params=kubectl run dns-test --image=busybox:1.36 --restart=Never --command --  sh -c 'nslookup kubernetes.default.svc.cluster.local' || true
    echo "Cleaning up test pods..."
kubectl delete pod dns-test --ignore-not-found
] ***
ok: [sopnode-f1]

TASK [setup/k8s/cluster_create : Generate join command _raw_params=kubeadm token create --print-join-command] ***
changed: [sopnode-f1]

TASK [setup/k8s/cluster_create : Save join command locally content={{ join_cmd.stdout }}, dest=.kubeadm_join_command.txt, mode=0644] ***
changed: [sopnode-f1 -> localhost]

PLAY [Debug type of bridge_enabled on RAN node] ********************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f2]

TASK [Show bridge_enabled and ran_node_name msg=["value = {{ bridge_enabled | default('UNDEFINED') }}", 'type = {{ bridge_enabled | default(None) | type_debug }}', "ran_node_name value = {{ ran_node_name | default('UNDEFINED') }}"]] ***
ok: [sopnode-f2] => {}

MSG:

['value = True', 'type = bool', 'ran_node_name value = sopnode-f2']

PLAY [Remove Control-Plane NoSchedule taint on CORE node] **********************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f1]

TASK [setup/k8s/remove_taint : Remove NoSchedule taint from control-plane node _raw_params=kubectl taint nodes {{ inventory_hostname }} node-role.kubernetes.io/control-plane:NoSchedule-
] ***
changed: [sopnode-f1]

PLAY [Join Kubernetes Cluster on Workers] **************************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/cluster_join : RESTART_K8S_JOIN _raw_params=noop] **************

TASK [setup/k8s/cluster_join : RESTART_K8S_JOIN _raw_params=noop] **************

TASK [setup/k8s/cluster_join : Check if node has leftover Kubernetes data path=/etc/kubernetes] ***
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/cluster_join : Reset node if Kubernetes data exists _raw_params=kubeadm reset -f] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Remove all Kubernetes and CNI state path={{ item }}, state=absent] ***
changed: [sopnode-f2] => (item=/etc/kubernetes)
changed: [sopnode-w3] => (item=/etc/kubernetes)
changed: [sopnode-f2] => (item=/etc/cni)
changed: [sopnode-w3] => (item=/etc/cni)
changed: [sopnode-f2] => (item=/var/lib/kubelet)
changed: [sopnode-w3] => (item=/var/lib/kubelet)
ok: [sopnode-f2] => (item=/var/lib/cni)
ok: [sopnode-w3] => (item=/var/lib/cni)
ok: [sopnode-f2] => (item=/run/flannel)
ok: [sopnode-w3] => (item=/run/flannel)

TASK [setup/k8s/cluster_join : Restart containerd name=containerd, state=restarted] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Wait for containerd to settle seconds=5] ********
Pausing for 5 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [sopnode-f2]

TASK [setup/k8s/cluster_join : Ensure CNI directories exist path={{ item }}, state=directory, mode=0755] ***
changed: [sopnode-f2] => (item=/etc/cni/net.d)
changed: [sopnode-w3] => (item=/etc/cni/net.d)
ok: [sopnode-f2] => (item=/opt/cni/bin)
ok: [sopnode-w3] => (item=/opt/cni/bin)
changed: [sopnode-f2] => (item=/var/lib/cni)
changed: [sopnode-w3] => (item=/var/lib/cni)

TASK [setup/k8s/cluster_join : Download CNI plugins (via curl, plus fiable) creates=/tmp/cni-plugins.tgz, _raw_params=curl -fsSL -o /tmp/cni-plugins.tgz  https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz
] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Extract CNI plugins src=/tmp/cni-plugins.tgz, dest=/opt/cni/bin, remote_src=True] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Fix permissions path=/opt/cni/bin, owner=root, group=root, mode=0755, recurse=True] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Verify essential CNI binaries exist path=/opt/cni/bin/{{ item }}] ***
ok: [sopnode-f2] => (item=loopback)
ok: [sopnode-w3] => (item=loopback)
ok: [sopnode-f2] => (item=bridge)
ok: [sopnode-w3] => (item=bridge)
ok: [sopnode-f2] => (item=portmap)
ok: [sopnode-w3] => (item=portmap)

TASK [setup/k8s/cluster_join : Fail if essential CNI plugins are missing msg=âŒ CNI plugin {{ item.item }} missing on node {{ inventory_hostname }}] ***
skipping: [sopnode-f2] => (item={'changed': False, 'stat': {'exists': True, 'path': '/opt/cni/bin/loopback', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': True, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 3750882, 'inode': 25637, 'dev': 29, 'nlink': 1, 'atime': 1770932890.5726855, 'mtime': 1718639477.0, 'ctime': 1770932890.5886853, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 7328, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'pw_name': 'root', 'gr_name': 'root', 'checksum': '26a08877aac964dffb49abbc29218beea36439ab', 'mimetype': 'unknown', 'charset': 'unknown', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/opt/cni/bin/loopback', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'item': 'loopback', 'ansible_loop_var': 'item'}) 
skipping: [sopnode-f2] => (item={'changed': False, 'stat': {'exists': True, 'path': '/opt/cni/bin/bridge', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': True, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 4788191, 'inode': 25645, 'dev': 29, 'nlink': 1, 'atime': 1770932890.692685, 'mtime': 1718639476.0, 'ctime': 1770932890.7166848, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 9352, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'pw_name': 'root', 'gr_name': 'root', 'checksum': 'f69057a064b4b4581e926b5cb710481bc7846021', 'mimetype': 'unknown', 'charset': 'unknown', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/opt/cni/bin/bridge', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'item': 'bridge', 'ansible_loop_var': 'item'}) 
skipping: [sopnode-f2] => (item={'changed': False, 'stat': {'exists': True, 'path': '/opt/cni/bin/portmap', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': True, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 4228332, 'inode': 25650, 'dev': 29, 'nlink': 1, 'atime': 1770932890.8006845, 'mtime': 1718639475.0, 'ctime': 1770932890.8206844, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 8264, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'pw_name': 'root', 'gr_name': 'root', 'checksum': '5083819e4ee619ee45cbc945472a0359bfa414b5', 'mimetype': 'unknown', 'charset': 'unknown', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/opt/cni/bin/portmap', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'item': 'portmap', 'ansible_loop_var': 'item'}) 
skipping: [sopnode-f2]
skipping: [sopnode-w3] => (item={'changed': False, 'stat': {'exists': True, 'path': '/opt/cni/bin/loopback', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': True, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 3750882, 'inode': 25627, 'dev': 29, 'nlink': 1, 'atime': 1770932890.883429, 'mtime': 1718639477.0, 'ctime': 1770932890.9074311, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 7328, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'pw_name': 'root', 'gr_name': 'root', 'checksum': '26a08877aac964dffb49abbc29218beea36439ab', 'mimetype': 'unknown', 'charset': 'unknown', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/opt/cni/bin/loopback', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'item': 'loopback', 'ansible_loop_var': 'item'}) 
skipping: [sopnode-w3] => (item={'changed': False, 'stat': {'exists': True, 'path': '/opt/cni/bin/bridge', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': True, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 4788191, 'inode': 25635, 'dev': 29, 'nlink': 1, 'atime': 1770932891.0354416, 'mtime': 1718639476.0, 'ctime': 1770932891.0674443, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 9352, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'pw_name': 'root', 'gr_name': 'root', 'checksum': 'f69057a064b4b4581e926b5cb710481bc7846021', 'mimetype': 'unknown', 'charset': 'unknown', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/opt/cni/bin/bridge', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'item': 'bridge', 'ansible_loop_var': 'item'}) 
skipping: [sopnode-w3] => (item={'changed': False, 'stat': {'exists': True, 'path': '/opt/cni/bin/portmap', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': True, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 4228332, 'inode': 25640, 'dev': 29, 'nlink': 1, 'atime': 1770932891.171453, 'mtime': 1718639475.0, 'ctime': 1770932891.195455, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 8264, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'pw_name': 'root', 'gr_name': 'root', 'checksum': '5083819e4ee619ee45cbc945472a0359bfa414b5', 'mimetype': 'unknown', 'charset': 'unknown', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/opt/cni/bin/portmap', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'item': 'portmap', 'ansible_loop_var': 'item'}) 
skipping: [sopnode-w3]

TASK [setup/k8s/cluster_join : Ensure /root/.kube exists path=/root/.kube, state=directory, mode=0755] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Fetch admin.conf from master to control machine src=/etc/kubernetes/admin.conf, dest=./admin.conf, flat=True] ***
skipping: [sopnode-w3]
changed: [sopnode-f2 -> sopnode-f1]

TASK [setup/k8s/cluster_join : Copy admin.conf to worker node src=./admin.conf, dest=/root/.kube/config, mode=0644] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Ensure master CORE node is resolvable path=/etc/hosts, line={{ hostvars[groups['core_node'][0]].ip }} {{ groups['core_node'][0] }}] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Set kubelet RAN node IP explicitly dest=/etc/default/kubelet, content=KUBELET_EXTRA_ARGS=--node-ip={{ hostvars[inventory_hostname].ip }}
] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Restart kubelet name=kubelet, state=restarted] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Ensure kube-proxy directory exists path=/var/lib/kube-proxy, state=directory, mode=0755] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Check if kube-proxy config exists on disk path=/etc/kubernetes/config.conf] ***
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/cluster_join : Create symlink to kube-proxy config if it exists src=/etc/kubernetes/config.conf, dest=/var/lib/kube-proxy/kubeproxy-config.yaml, state=link, force=True] ***
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/k8s/cluster_join : Warn user if kube-proxy config is missing msg="âš ï¸ kube-proxy configuration not found on disk; relying on DaemonSet ConfigMap. Pod kube-proxy should start normally if running as DaemonSet."
] ***
ok: [sopnode-f2] => {}

MSG:

"âš ï¸ kube-proxy configuration not found on disk; relying on DaemonSet ConfigMap. Pod kube-proxy should start normally if running as DaemonSet."

ok: [sopnode-w3] => {}

MSG:

"âš ï¸ kube-proxy configuration not found on disk; relying on DaemonSet ConfigMap. Pod kube-proxy should start normally if running as DaemonSet."


TASK [setup/k8s/cluster_join : Remove stale node entry from cluster _raw_params=kubectl delete node {{ inventory_hostname }} --ignore-not-found
] ***
changed: [sopnode-f2 -> sopnode-f1]
changed: [sopnode-w3 -> sopnode-f1]

TASK [setup/k8s/cluster_join : Read join command kubeadm_join_command={{ lookup('file', '.kubeadm_join_command.txt') }}] ***
ok: [sopnode-f2]
ok: [sopnode-w3]

TASK [setup/k8s/cluster_join : Debug join command var=kubeadm_join_command] ****
ok: [sopnode-f2] => {
    "kubeadm_join_command": "kubeadm join 172.28.2.76:6443 --token 0733dm.2glgofl7a911nv8n --discovery-token-ca-cert-hash sha256:7d2466ae7675d7b9e386bf3f95ff128e209f0cf9b2a74831c44637da1ac506d4"
}
ok: [sopnode-w3] => {
    "kubeadm_join_command": "kubeadm join 172.28.2.76:6443 --token 0733dm.2glgofl7a911nv8n --discovery-token-ca-cert-hash sha256:7d2466ae7675d7b9e386bf3f95ff128e209f0cf9b2a74831c44637da1ac506d4"
}

TASK [setup/k8s/cluster_join : Join node to the cluster _raw_params={{ kubeadm_join_command }} --ignore-preflight-errors=SystemVerification --v=5] ***
changed: [sopnode-f2]
changed: [sopnode-w3]

TASK [setup/k8s/cluster_join : Wait until CoreDNS pods are Ready _raw_params=set -e
echo "Checking CoreDNS pods readiness on control-plane node..."
kubectl wait -n kube-system  --for=condition=Ready pod -l k8s-app=kube-dns  --timeout=300s
] ***
ok: [sopnode-f2 -> sopnode-f1]
ok: [sopnode-w3 -> sopnode-f1]

TASK [setup/k8s/cluster_join : Debug CoreDNS status if not ready _raw_params=echo "CoreDNS pods not ready yet â€” showing status:"
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
echo
echo "--- Describe output ---"
kubectl describe pods -n kube-system -l k8s-app=kube-dns | tail -n 50
] ***
skipping: [sopnode-f2]
skipping: [sopnode-w3]

TASK [setup/k8s/cluster_join : Test CoreDNS resolution from core and ran nodes _raw_params=set -eux
for NODE in {{ groups['core_node'][0] }} {{ groups['ran_node'][0] }}; do
  echo "=== Testing DNS on $NODE ==="
  kubectl run dns-test-$NODE    --image=busybox:1.36    --restart=Never    --overrides='{"spec": {"nodeName": "'"${NODE}"'"}}'    --command -- sh -c 'nslookup kubernetes.default.svc.cluster.local' || true
done

echo "Cleaning up test pods..."
kubectl delete pod dns-test-{{ groups['core_node'][0] }} --ignore-not-found
kubectl delete pod dns-test-{{ groups['ran_node'][0] }} --ignore-not-found
] ***
ok: [sopnode-f2 -> sopnode-f1]
ok: [sopnode-w3 -> sopnode-f1]

PLAY [Add NoSchedule taint on RAN node (reserved node for RAN pods)] ***********

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f1]

TASK [setup/k8s/add_taint : Force Taint node sopnode-f2 with NoSchedule _raw_params=kubectl taint node {{ target_node }} test-eviction=true:NoSchedule --overwrite
] ***
changed: [sopnode-f1]

TASK [setup/k8s/add_taint : Clean up core pods from node sopnode-f2 _raw_params=kubectl delete pods -n open5gs --field-selector spec.nodeName={{ target_node }} --ignore-not-found=true
] ***
changed: [sopnode-f1]

PLAY [Install Multus / CNAO] ***************************************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f1]

TASK [setup/cni : RESTART_K8S_ADDS_ON _raw_params=noop] ************************

TASK [setup/cni : Ensure Open vSwitch is installed name=openvswitch-switch, state=present, update_cache=True] ***
ok: [sopnode-f1]

TASK [setup/cni : Create OVS bridges for ovs-cni _raw_params=ovs-vsctl --may-exist add-br {{ item }}] ***
changed: [sopnode-f1] => (item=n2br)
changed: [sopnode-f1] => (item=n3br)
changed: [sopnode-f1] => (item=n4br)

TASK [setup/cni : Render NetworkAddonsConfig manifest locally src=network-addons-config.yaml.j2, dest=/tmp/network-addons-config.yaml] ***
changed: [sopnode-f1]

TASK [setup/cni : Render NetworkAddonsConfig manifest locally src=network-addons-config-full-oai.yaml.j2, dest=/tmp/network-addons-config.yaml] ***
skipping: [sopnode-f1]

TASK [setup/cni : Apply Cluster-Network-Addons Operator manifests _raw_params=kubectl apply -f {{ item }}
] ***
changed: [sopnode-f1] => (item=https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.89.1/namespace.yaml)
changed: [sopnode-f1] => (item=https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.89.1/network-addons-config.crd.yaml)
changed: [sopnode-f1] => (item=https://github.com/kubevirt/cluster-network-addons-operator/releases/download/v0.89.1/operator.yaml)

TASK [setup/cni : Wait for cluster-network-addons namespace to exist api_version=v1, kind=Namespace, name=cluster-network-addons, kubeconfig=/etc/kubernetes/admin.conf] ***
ok: [sopnode-f1]

TASK [setup/cni : Wait for cluster-network-addons-operator to be ready _raw_params=kubectl -n cluster-network-addons rollout status deployment/cluster-network-addons-operator --timeout=300s
] ***
ok: [sopnode-f1]

TASK [setup/cni : Apply NetworkAddonsConfig from /tmp state=present, src=/tmp/network-addons-config.yaml, kubeconfig=/etc/kubernetes/admin.conf] ***
ok: [sopnode-f1]

TASK [setup/cni : Wait for NetworkAddonsConfig to reach Available condition _raw_params=set -e
for i in $(seq 1 60); do
  phase=$(kubectl -n cluster-network-addons get networkaddonsconfig cluster -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "Unknown")
  echo "â³ Waiting for NetworkAddonsConfig to be Available (attempt $i): $phase"
  if [ "$phase" = "True" ]; then
    echo "âœ… NetworkAddonsConfig is Available"
    exit 0
  fi
  sleep 10
done
echo "âŒ NetworkAddonsConfig did not become Available in time"
exit 1
] ***
ok: [sopnode-f1]

TASK [setup/storage : Add OpenEBS Helm repository name=openebs, repo_url=https://openebs.github.io/charts] ***
changed: [sopnode-f1]

TASK [setup/storage : Update Helm repositories _raw_params=helm repo update] ***
changed: [sopnode-f1]

TASK [setup/storage : Install OpenEBS using Helm name=openebs, chart_ref=openebs/openebs, namespace=openebs, create_namespace=True, values={'ndm': {'nodeSelector': {'node-role.kubernetes.io/control-plane': ''}}, 'localprovisioner': {'nodeSelector': {'node-role.kubernetes.io/control-plane': ''}}}] ***
changed: [sopnode-f1]

TASK [setup/storage : Patch OpenEBS storage class to be default _raw_params=kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
] ***
changed: [sopnode-f1]

PLAY [Apply specific tuning on RAN Node] ***************************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : RESTART_RAN_K8S _raw_params=noop] *************

TASK [setup/k8s/k8s_cpu_tuning : Ensure CPUAffinity 5-15 is set in system.conf path=/etc/systemd/system.conf, regexp=^CPUAffinity=, line=CPUAffinity=5-15, state=present] ***
changed: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Reload systemd config _raw_params=systemctl daemon-reexec] ***
changed: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Add Kubernetes and RT settings to sysctl.conf path=/etc/sysctl.conf, insertafter=EOF, block=## For kubernetes
fs.inotify.max_user_instances=65536
## Realtime kernel settings
kernel.sched_rt_runtime_us=-1
kernel.timer_migration=0
, marker=# {mark} RAN tuning] ***
changed: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Apply sysctl changes (reload) _raw_params=sysctl -p] ***
ok: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Ensure python3 and PyYAML are installed name=['python3', 'python3-yaml'], state=present, update_cache=True] ***
ok: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Merge CPU-only kubelet config safely _raw_params=python3 - <<'EOF'
import yaml, os
cfg = "/var/lib/kubelet/config.yaml"

# 1. Base CPU Tuning Patch
patch = {
    "cgroupDriver": "systemd",
    "cpuManagerPolicy": "static",
    "cpuManagerPolicyOptions": {"full-pcpus-only": "true"},
    "reservedSystemCPUs": "0-15",
    "failSwapOn": False,
    "featureGates": {
        "CPUManager": True,
        "CPUManagerPolicyOptions": True
    }
}

# 2. Add Storage settings if RAN and CORE are on the same node (passed via Jinja2)
# This prevents DiskPressure on Live Ubuntu images
is_shared_node = {{ True if (ran_node_name == core_node_name) else False }}

if is_shared_node:
    patch["rootDir"] = "/var/lib/containerd"
    patch["evictionHard"] = {
        "nodefs.available": "2%",
        "imagefs.available": "2%"
    }
    patch["evictionMinimumReclaim"] = {
        "nodefs.available": "500Mi"
    }

if not os.path.exists(cfg):
    print("ERROR: Kubelet config not found")
    exit(1)

with open(cfg) as f: 
    data = yaml.safe_load(f) or {}

# 3. Merge featureGates separately to avoid overwriting existing ones
if "featureGates" in patch:
    data.setdefault("featureGates", {}).update(patch["featureGates"])

# 4. Merge other keys
for k, v in patch.items():
    if k != "featureGates":
        data[k] = v

# 5. Atomic write to avoid corrupted config
with open(cfg + ".tmp", "w") as f:
    yaml.safe_dump(data, f, default_flow_style=False, sort_keys=False)

os.replace(cfg + ".tmp", cfg)
print("APPLIED")
EOF
] ***
changed: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Restart kubelet name=kubelet, state=restarted, daemon_reload=True] ***
changed: [sopnode-f2]

TASK [setup/k8s/k8s_cpu_tuning : Remove kubelet CPU manager checkpoint file path=/var/lib/kubelet/cpu_manager_state, state=absent] ***
changed: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Ensure CPUAffinity is set in system.conf path=/etc/systemd/system.conf, regexp=^CPUAffinity=, line=CPUAffinity=0-3, state=present] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Reload systemd config _raw_params=systemctl daemon-reexec] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Add Kubernetes and RT settings to sysctl.conf path=/etc/sysctl.conf, insertafter=EOF, block=## For kubernetes
fs.inotify.max_user_instances=65536
## Realtime kernel settings
kernel.sched_rt_runtime_us=-1
kernel.timer_migration=0
, marker=# {mark} RAN tuning] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Apply sysctl changes (reload) _raw_params=sysctl -p] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Ensure python3 and PyYAML are installed name=['python3', 'python3-yaml'], state=present, update_cache=True] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Merge CPU-only kubelet config safely _raw_params=python3 - <<'EOF'
import yaml, os

cfg = "/var/lib/kubelet/config.yaml"

# CPU tuning patch â€” ONLY these fields will be enforced
patch = {
  "cgroupDriver": "systemd",
  "cpuManagerPolicy": "static",
  "cpuManagerPolicyOptions": {"full-pcpus-only": "true"},
  "reservedSystemCPUs": "0-5",
  "topologyManagerPolicy": "best-effort",
  "failSwapOn": False,
  "featureGates": {
    "CPUManager": True,
    "CPUManagerPolicyOptions": True,
  }
}

if not os.path.exists(cfg):
  exit(0)

with open(cfg) as f:
  data = yaml.safe_load(f) or {}

# Deep merge featureGates only
data.setdefault("featureGates", {})
data["featureGates"].update(patch["featureGates"])

# Shallow merge non-dict keys
for k, v in patch.items():
  if k != "featureGates":
    data[k] = v

with open(cfg + ".tmp", "w") as f:
  yaml.safe_dump(data, f, default_flow_style=False, sort_keys=False)
os.replace(cfg + ".tmp", cfg)
print("APPLIED")
EOF
] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Remove kubelet CPU manager checkpoint path=/var/lib/kubelet/cpu_manager_state, state=absent] ***
skipping: [sopnode-f2]

TASK [setup/k8s/benetel_specific_tuning_f3 : Restart kubelet name=kubelet, state=restarted, daemon_reload=True] ***
skipping: [sopnode-f2]

PLAY [Deploy 5G-Monarch] *******************************************************

TASK [Gathering Facts ] ********************************************************
ok: [sopnode-w3]

TASK [monarch/deploy : Ensure /mnt/data mount point exists path=/mnt/data, state=directory, mode=0755] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Mount /dev/sda1 to /mnt/data path=/mnt/data, src=/dev/{{ storage }}, fstype=ext4, state=mounted] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Create MongoDB data directory with correct ownership path=/mnt/data/mongodb, state=directory, owner=1001, group=1001, mode=0755] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Create MinIO data directory with correct ownership path=/mnt/data/minio, state=directory, owner=1000, group=1000, mode=0755] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Create Thanos data directory path=/mnt/data/thanos, state=directory, mode=0755] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Create Prometheus data directory path=/mnt/data/prometheus, state=directory, owner=65534, group=65534, mode=0755] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Clone 5G-Monarch repo repo={{ repo_url }}, dest={{ repo_dest }}, version={{ branch }}, update=True, force=True] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Label sopnode-w3 for Monarch scheduling _raw_params=kubectl label node {{ groups['monitor_node'][0] }} monarch-node=true --overwrite] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Ensure Kyverno namespace exists api_version=v1, kind=Namespace, name=kyverno, state=present] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Check if Kyverno release exists _raw_params=helm status kyverno -n kyverno] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Add Kyverno Helm repo _raw_params=helm repo add kyverno https://kyverno.github.io/kyverno/] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Update Helm repos _raw_params=helm repo update] *********
changed: [sopnode-w3]

TASK [monarch/deploy : Install Kyverno using Helm _raw_params=helm upgrade --install kyverno kyverno/kyverno -n kyverno --create-namespace
] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Copy Kyverno policy dest=/tmp/monarch-force-node.yaml, content=apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: monarch-force-node
spec:
  rules:
    - name: add-monarch-node-selector
      match:
        resources:
          kinds: ["Pod"]
          namespaces: ["monarch"]
      mutate:
        patchStrategicMerge:
          spec:
            nodeSelector:
              monarch-node: "true"
] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Apply Kyverno policy _raw_params=kubectl apply -f /tmp/monarch-force-node.yaml] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Ensure monarch namespace exists state=present, definition={'apiVersion': 'v1', 'kind': 'Namespace', 'metadata': {'name': '{{ monarch_ns }}'}}] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Patch namespace with default nodeSelector to force scheduling on labeled node state=patched, kind=Namespace, name={{ monarch_ns }}, merge_type=merge, definition={'metadata': {'annotations': {'scheduler.kubernetes.io/node-selector': 'monarch-node=true'}}}] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Get IP of node sopnode-w3 _raw_params=kubectl get node {{ groups['monitor_node'][0] }} -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}'] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Set monarch_node_ip fact monarch_node_ip={{ monarch_node_ip_result.stdout }}] ***
ok: [sopnode-w3]

TASK [monarch/deploy : Patch NODE_IP in .env path={{ repo_dest }}/.env, regexp=^NODE_IP=.*, line=NODE_IP="{{ monarch_node_ip }}", create=True] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Install Python requirements requirements={{ repo_dest }}/requirements.txt, executable=pip3] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Deploy Monarch External Components (SO, NFVO) msg=Starting deployment of Monarch external components...] ***
ok: [sopnode-w3] => {}

MSG:

Starting deployment of Monarch external components...

TASK [monarch/deploy : Deploy external Monarch components (SO / NFVO) chdir={{ repo_dest }}/{{ item }}, _raw_params=./install.sh] ***
changed: [sopnode-w3] => (item=service_orchestrator)
changed: [sopnode-w3] => (item=nfv_orchestrator)

TASK [monarch/deploy : Append nodeSelector for MongoDB (exact indent, end of file) path={{ repo_dest }}/data_store/mongodb/mongodb-deployment.yaml, marker=# {mark} ANSIBLE MANAGED BLOCK: NodeSelector for MongoDB, insertafter=EOF, block=# fixing indentation
      nodeSelector:
        kubernetes.io/hostname: {{ groups['monitor_node'][0] }}
] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Deploy Monarch Core Components (Data, Monitoring)" msg=Starting deployment of Monarch core components...] ***
ok: [sopnode-w3] => {}

MSG:

Starting deployment of Monarch core components...

TASK [monarch/deploy : Install data-store stack chdir={{ repo_dest }}/data_store, _raw_params=./install.sh] ***
changed: [sopnode-w3]

TASK [monarch/deploy : Wait for data-store pods _raw_params=wait_pod.yml] ******
included: /home/yamami/5g_ansible_latest/roles/monarch/deploy/tasks/wait_pod.yml for sopnode-w3 => (item={'selector': 'app=minio', 'label': 'minio'})
included: /home/yamami/5g_ansible_latest/roles/monarch/deploy/tasks/wait_pod.yml for sopnode-w3 => (item={'selector': 'app.kubernetes.io/name=mongodb', 'label': 'mongodb'})

TASK [monarch/deploy : Wait for minio pod to be Ready kind=Pod, namespace={{ monarch_ns }}, label_selectors=['{{ item.selector }}']] ***
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (30 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (29 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (28 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (27 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (26 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (25 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (24 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (23 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (22 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (21 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (20 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (19 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (18 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (17 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (16 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (15 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (14 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (13 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (12 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (11 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (10 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (9 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (8 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (7 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (6 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (5 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (4 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (3 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (2 retries left).
FAILED - RETRYING: [sopnode-w3]: Wait for minio pod to be Ready (1 retries left).
fatal: [sopnode-w3]: FAILED! => {
    "api_found": true,
    "attempts": 30,
    "changed": false,
    "resources": []
}

PLAY RECAP *********************************************************************
localhost                  : ok=7    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
sopnode-f1                 : ok=133  changed=76   unreachable=0    failed=0    skipped=32   rescued=0    ignored=0   
sopnode-f2                 : ok=119  changed=64   unreachable=0    failed=0    skipped=57   rescued=0    ignored=1   
sopnode-w3                 : ok=131  changed=73   unreachable=0    failed=1    skipped=28   rescued=0    ignored=0   

âŒ Command failed with exit code 2: ansible-playbook -i ./inventory/default/hosts.ini -e fiveg_profile=default  playbooks/deploy.yml
