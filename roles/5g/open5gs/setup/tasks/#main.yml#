---
- name: RESTART_OPEN5GS_SETUP
  ansible.builtin.meta: noop

- name: Reload and restart kubelet
  ansible.builtin.shell: |
    systemctl daemon-reload
    systemctl restart kubelet
  become: true
  changed_when: false

- name: Restart CoreDNS pods after kubelet restart
  ansible.builtin.shell: |
    kubectl -n kube-system delete pod -l k8s-app=kube-dns --ignore-not-found
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  changed_when: false

- name: Wait for CoreDNS to be Ready
  ansible.builtin.shell: |
    kubectl wait -n kube-system --for=condition=Ready pod -l k8s-app=kube-dns --timeout=180s
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: wait_coredns
  retries: 3
  delay: 10
  until: wait_coredns.rc == 0
  changed_when: false

# --- Test DNS ---
- name: Test CoreDNS resolution from core node
  become: true
  delegate_to: "{{ groups['core_node'][0] }}"
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  ansible.builtin.shell: |
    set -eux
    for NODE in {{ groups['core_node'][0] }}; do
      echo "=== Testing DNS on $NODE ==="
      kubectl run dns-test-$NODE \
        --image=busybox:1.36 \
        --restart=Never \
        --overrides='{"spec": {"nodeName": "'"${NODE}"'"}}' \
        --command -- sh -c 'nslookup kubernetes.default.svc.cluster.local' || true
    done

    echo "Cleaning up test pods..."
    kubectl delete pod dns-test-{{ groups['core_node'][0] }} --ignore-not-found
  changed_when: false

- name: Label {{ groups['core_node'][0] }} for Open5GS Core scheduling
  command: kubectl label node {{ groups['core_node'][0] }} open5gs-core-node=true --overwrite
  changed_when: false

# === KYVERNO INSTALLATION START =====================================================

- name: Add and update Kyverno Helm repo
  ansible.builtin.shell: |
    helm repo add kyverno https://kyverno.github.io/kyverno/ || true
    helm repo update
  args:
    executable: /bin/bash

- name: Ensure Kyverno namespace exists
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: kyverno

# --- Optional cleanup if Kyverno partially installed ---
- name: Uninstall previous Kyverno (safe cleanup)
  ansible.builtin.shell: |
    helm uninstall kyverno -n kyverno || true
    kubectl delete crds -l app.kubernetes.io/name=kyverno --ignore-not-found=true
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  ignore_errors: true

# --- Flannel CNI Check ---
- name: Wait for Flannel pods to be Ready
  ansible.builtin.shell: |
    kubectl wait -n kube-flannel --for=condition=Ready pod -l app=flannel --timeout=180s
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: wait_flannel
  retries: 5
  delay: 10
  until: wait_flannel.rc == 0
  changed_when: false

- name: Wait for Flannel CNI interface (cni0) to exist on core node
  ansible.builtin.shell: |
    set -e
    echo "Checking if cni0 exists on {{ item }}..."
    for i in {1..30}; do
      if ip link show cni0 2>/dev/null; then
        echo "âœ… cni0 exists on {{ item }}"
        exit 0
      fi
      echo "â³ Waiting for cni0 to appear on {{ item }}..."
      sleep 5
    done
    echo "âŒ cni0 not found on {{ item }}" >&2
    exit 1
  delegate_to: "{{ item }}"
  loop: "{{ groups['core_node'] }}"
  retries: 3
  delay: 5
  register: wait_cni
  changed_when: false

# Ensure cluster networking and api endpoints are ready before Kyverno install
- name: Wait for all kube-proxy pods to be Ready
  ansible.builtin.shell: |
    set -e
    for i in $(seq 1 30); do
      not_ready=$(kubectl -n kube-system get pods -l k8s-app=kube-proxy -o jsonpath='{range .items[*]}{.metadata.name}:{range .status.containerStatuses[*]}{.ready};{end}{"\n"}{end}' | grep -c false || true)
      if [ "$not_ready" -eq 0 ]; then
        echo "âœ… All kube-proxy pods are ready"
        exit 0
      fi
      echo "â³ Waiting for kube-proxy pods ($i/30)..."
      sleep 5
    done
    echo "âŒ kube-proxy pods not ready after 30 retries"
    kubectl -n kube-system get pods -o wide
    exit 1
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  changed_when: false

- name: Wait for kubernetes service endpoint to exist (10.96.0.1)
  ansible.builtin.shell: |
    set -eu
    tries=0
    until kubectl -n default get endpoints kubernetes -o jsonpath='{.subsets}' | grep -q '.'; do
      tries=$((tries+1))
      if [ $tries -gt 30 ]; then
        echo "no endpoints for kubernetes service after timeout" >&2
        exit 1
      fi
      sleep 2
    done
    kubectl -n default get endpoints kubernetes -o wide
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  args:
    executable: /bin/bash
  changed_when: false
  register: kubernetes_endpoints

- name: Verify core node has a route to service IP 10.96.0.1
  ansible.builtin.shell: |
    if ip route get 10.96.0.1 >/dev/null 2>&1; then
      ip route get 10.96.0.1
      exit 0
    else
      echo "no route to 10.96.0.1 on $(hostname)" >&2
      exit 2
    fi
  delegate_to: "{{ item }}"
  loop: "{{ groups['core_node'] }}"
  register: route_check
  failed_when: false
  changed_when: false

- name: Debug route check failures if any node lacks route to 10.96.0.1
  ansible.builtin.debug:
    msg: "Node {{ item.item }}: rc={{ item.rc }} stdout='{{ item.stdout }}' stderr='{{ item.stderr }}'"
  loop: "{{ route_check.results }}"
  when: item.rc != 0

- name: Check if no route to 10.96.0.1 on core node 
  ansible.builtin.fail:
    msg: "Core node does not have a route to 10.96.0.1; Kyverno install deferred."
  when: route_check.results | selectattr('rc','ne',0) | list | length > 0

# --- Install CRDs first (fixes the init crash) ---
- name: Download Kyverno chart locally
  ansible.builtin.shell: |
    rm -rf /tmp/kyverno
    mkdir -p /tmp/kyverno
    helm pull kyverno/kyverno --version 3.5.2 --untar --untardir /tmp/kyverno
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

- name: Find actual untarred Kyverno folder
  ansible.builtin.find:
    paths: /tmp/kyverno
    file_type: directory
    patterns: "kyverno*"
  register: kyverno_chart_folder

- name: Fail if no Kyverno chart folder found
  ansible.builtin.fail:
    msg: "No Kyverno chart folder found in /tmp/kyverno. Helm pull may have failed."
  when: kyverno_chart_folder.matched == 0

- name: Install Kyverno CRDs (if present) before helm install
  ansible.builtin.shell: |
    set -e
    chart_dir="{{ kyverno_chart_folder.files[0].path }}"
    if [ -d "$chart_dir/crds" ]; then
      kubectl apply -f "$chart_dir/crds"
    else
      echo "No crds directory in $chart_dir, skipping CRD apply"
    fi
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

- name: Install Kyverno Helm chart (Helm manages CRDs)
  ansible.builtin.shell: |
    helm upgrade --install kyverno {{ kyverno_chart_folder.files[0].path }} \
      -n kyverno --create-namespace --wait=false
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

# --- Wait for Kyverno controllers to be ready ---
- name: Wait for Kyverno controller pods to be Ready
  ansible.builtin.shell: |
    set -e
    kubectl -n kyverno wait --for=condition=Available deployment/kyverno-admission-controller --timeout=180s
    kubectl -n kyverno wait --for=condition=Available deployment/kyverno-background-controller --timeout=180s
    kubectl -n kyverno wait --for=condition=Available deployment/kyverno-cleanup-controller --timeout=180s
    kubectl -n kyverno wait --for=condition=Available deployment/kyverno-reports-controller --timeout=180s
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: wait_kyverno
  retries: 5
  delay: 15
  until: wait_kyverno.rc == 0
  changed_when: false

# --- Status summary (replacement for debug block) ---
- name: Show Kyverno status summary
  ansible.builtin.shell: |
    echo "ðŸ”¹ Kyverno Deployments:"
    kubectl -n kyverno get deploy -o wide
    echo -e "\nðŸ”¹ Kyverno Pods:"
    kubectl -n kyverno get pods -o wide
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  changed_when: false

- name: Copy Kyverno policy to force Open5GS pods to {{ groups['core_node'][0] }}
  copy:
    dest: /tmp/force-open5gs-node.yaml
    content: |
      apiVersion: kyverno.io/v1
      kind: ClusterPolicy
      metadata:
        name: force-open5gs-to-core-node
      spec:
        rules:
          - name: add-open5gs-node-selector
            match:
              resources:
                kinds: ["Pod"]
                selector:
                  matchLabels:
                    app: open5gs
            mutate:
              patchStrategicMerge:
                spec:
                  nodeSelector:
                    open5gs-core-node: "true"

- name: Apply Kyverno policy
  command: kubectl apply -f /tmp/force-open5gs-node.yaml
  changed_when: false

