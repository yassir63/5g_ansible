---
- name: RESTART_5G_CORE
  ansible.builtin.meta: noop

# --- Correction kubelet + CoreDNS sans flag obsolète ---
#- name: Ensure kubelet uses correct CNI and DNS config
#  lineinfile:
#    path: /etc/default/kubelet
#    regexp: '^KUBELET_EXTRA_ARGS='
#    line: 'KUBELET_EXTRA_ARGS="--cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/c#ni/net.d --resolv-conf=/run/systemd/resolve/resolv.conf"'
#    create: yes
#    mode: '0644'

- name: Reload and restart kubelet
  ansible.builtin.shell: |
    systemctl daemon-reload
    systemctl restart kubelet
  become: true
  changed_when: false

- name: Restart CoreDNS pods after kubelet restart
  ansible.builtin.shell: |
    kubectl -n kube-system delete pod -l k8s-app=kube-dns --ignore-not-found
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  changed_when: false

- name: Wait for CoreDNS to be Ready
  ansible.builtin.shell: |
    kubectl wait -n kube-system --for=condition=Ready pod -l k8s-app=kube-dns --timeout=180s
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: wait_coredns
  retries: 3
  delay: 10
  until: wait_coredns.rc == 0
  changed_when: false

# --- Test DNS ---
- name: Test CoreDNS resolution from core and ran nodes
  become: true
  delegate_to: "{{ groups['core_node'][0] }}"
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  ansible.builtin.shell: |
    set -eux
    for NODE in {{ groups['core_node'][0] }} {{ groups['ran_node'][0] }}; do
      echo "=== Testing DNS on $NODE ==="
      kubectl run dns-test-$NODE \
        --image=busybox:1.36 \
        --restart=Never \
        --overrides='{"spec": {"nodeName": "'"${NODE}"'"}}' \
        --command -- sh -c 'nslookup kubernetes.default.svc.cluster.local' || true
    done

    echo "Cleaning up test pods..."
    kubectl delete pod dns-test-{{ groups['core_node'][0] }} --ignore-not-found
    kubectl delete pod dns-test-{{ groups['ran_node'][0] }} --ignore-not-found
  changed_when: false

- name: Label {{ groups['core_node'][0] }} for Open5GS Core scheduling
  command: kubectl label node {{ groups['core_node'][0] }} open5gs-core-node=true --overwrite
  changed_when: false

# === KYVERNO INSTALLATION =====================================================
- name: Add and update Kyverno Helm repo
  ansible.builtin.shell: |
    helm repo add kyverno https://kyverno.github.io/kyverno/ || true
    helm repo update
  args:
    executable: /bin/bash

- name: Ensure Kyverno namespace exists
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: kyverno

# --- Optional cleanup if Kyverno partially installed ---
- name: Uninstall previous Kyverno (safe cleanup)
  ansible.builtin.shell: |
    helm uninstall kyverno -n kyverno || true
    kubectl delete crds -l app.kubernetes.io/name=kyverno --ignore-not-found=true
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  ignore_errors: true

# --- Flannel CNI Check ---
- name: Wait for Flannel pods to be Ready on all nodes
  ansible.builtin.shell: |
    kubectl wait -n kube-flannel --for=condition=Ready pod -l app=flannel --timeout=180s
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: wait_flannel
  retries: 5
  delay: 10
  until: wait_flannel.rc == 0
  changed_when: false

- name: Wait for Flannel CNI interface (cni0) to exist on each node
  ansible.builtin.shell: |
    set -e
    echo "Checking if cni0 exists on {{ item }}..."
    for i in {1..30}; do
      if ip link show cni0 2>/dev/null; then
        echo "✅ cni0 exists on {{ item }}"
        exit 0
      fi
      echo "⏳ Waiting for cni0 to appear on {{ item }}..."
      sleep 5
    done
    echo "❌ cni0 not found on {{ item }}" >&2
    exit 1
  delegate_to: "{{ item }}"
  loop: "{{ groups['core_node'] + groups['ran_node'] }}"
  retries: 3
  delay: 5
  register: wait_cni
  changed_when: false

# --- Check connectivity before Kyverno install ---
- name: Wait for network connectivity from RAN node to API server
  become: true
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  ansible.builtin.shell: |
    echo "Testing API server reachability from {{ groups['ran_node'][0] }}"
    kubectl run net-test --image=busybox:1.36 --restart=Never \
      --overrides='{"spec": {"nodeName": "{{ groups['ran_node'][0] }}"}}' \
      --command -- sh -c 'wget -qO- --timeout=5 https://10.96.0.1:443 || true'
    kubectl delete pod net-test --ignore-not-found
  register: api_reach_test
  retries: 10
  delay: 10
  until: "'unable to resolve' not in api_reach_test.stdout and api_reach_test.rc == 0"
  changed_when: false

# --- Ensure cluster networking and api endpoints are ready before Kyverno install ---
- name: Wait for kube-proxy DaemonSet to be available
  ansible.builtin.shell: |
    kubectl -n kube-system wait --for=condition=Available daemonset/kube-proxy --timeout=180s || true
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  args:
    executable: /bin/bash
  changed_when: false
  register: wait_kubeproxy

- name: Wait for kube-proxy pods to be Ready on all nodes
  ansible.builtin.shell: |
    set -e
    for POD in $(kubectl -n kube-system get pods -l k8s-app=kube-proxy -o jsonpath='{range .items[*]}{.metadata.name} {"\n"}{end}'); do
      kubectl -n kube-system wait --for=condition=Ready pod/"$POD" --timeout=120s || true
    done
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  args:
    executable: /bin/bash
  changed_when: false
  register: wait_kubeproxy_pods

- name: Wait for kubernetes service endpoint to exist (10.96.0.1)
  ansible.builtin.shell: |
    set -eu
    tries=0
    until kubectl -n default get endpoints kubernetes -o jsonpath='{.subsets}' | grep -q '.'; do
      tries=$((tries+1))
      if [ $tries -gt 30 ]; then
        echo "no endpoints for kubernetes service after timeout" >&2
        exit 1
      fi
      sleep 2
    done
    kubectl -n default get endpoints kubernetes -o wide
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  args:
    executable: /bin/bash
  changed_when: false
  register: kubernetes_endpoints

- name: Verify nodes have a route to service IP 10.96.0.1 (check on each node)
  ansible.builtin.shell: |
    if ip route get 10.96.0.1 >/dev/null 2>&1; then
      ip route get 10.96.0.1
      exit 0
    else
      echo "no route to 10.96.0.1 on $(hostname)" >&2
      exit 2
    fi
  delegate_to: "{{ item }}"
  loop: "{{ groups['core_node'] + groups['ran_node'] }}"
  register: route_check
  failed_when: false
  changed_when: false

- name: Debug route check failures if any node lacks route to 10.96.0.1
  ansible.builtin.debug:
    msg: "Node {{ item.item }}: rc={{ item.rc }} stdout='{{ item.stdout }}' stderr='{{ item.stderr }}'"
  loop: "{{ route_check.results }}"
  when: item.rc != 0

- name: Fail early if no route to 10.96.0.1 on any node (network must be fixed first)
  ansible.builtin.fail:
    msg: "One or more nodes do not have a route to 10.96.0.1; Kyverno install deferred."
  when: route_check.results | selectattr('rc','ne',0) | list | length > 0

# --- Install CRDs first (fixes the init crash) ---
- name: Download Kyverno chart locally
  ansible.builtin.shell: |
    rm -rf /tmp/kyverno
    mkdir -p /tmp/kyverno
    helm pull kyverno/kyverno --version 3.5.2 --untar --untardir /tmp/kyverno
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

- name: Find actual untarred Kyverno folder
  ansible.builtin.find:
    paths: /tmp/kyverno
    file_type: directory
    patterns: "kyverno*"
  register: kyverno_chart_folder

- name: Fail if no Kyverno chart folder found
  ansible.builtin.fail:
    msg: "No Kyverno chart folder found in /tmp/kyverno. Helm pull may have failed."
  when: kyverno_chart_folder.matched == 0

- name: Install Kyverno CRDs (if present) before helm install
  ansible.builtin.shell: |
    set -e
    chart_dir="{{ kyverno_chart_folder.files[0].path }}"
    if [ -d "$chart_dir/crds" ]; then
      kubectl apply -f "$chart_dir/crds"
    else
      echo "No crds directory in $chart_dir, skipping CRD apply"
    fi
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

- name: Install Kyverno Helm chart (Helm manages CRDs)
  ansible.builtin.shell: |
    helm upgrade --install kyverno {{ kyverno_chart_folder.files[0].path }} \
      -n kyverno --create-namespace --wait=false
  args:
    executable: /bin/bash
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

- name: Wait for Kyverno deployment to be ready
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: Deployment
    namespace: kyverno
    name: kyverno
  register: kyverno_status
  until:
    - kyverno_status.resources | length > 0
    - kyverno_status.resources[0].status.readyReplicas is defined
    - kyverno_status.resources[0].status.readyReplicas >= 1
  retries: 40
  delay: 15
  ignore_errors: yes

- name: Debug Kyverno deployment and pods if not ready
  block:
    - debug:
        var: kyverno_status.resources[0].status

    - name: List Kyverno pods
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: kyverno
      register: kyverno_pods

    - debug:
        var: kyverno_pods.resources

    - name: Fetch init container logs for Kyverno pods
      ansible.builtin.shell: |
        kubectl logs -n kyverno {{ item.0.metadata.name }} -c {{ item.1.name }} --tail=50
      loop: "{{ (kyverno_pods.resources | d([])) | subelements('spec.initContainers', skip_missing=True) }}"
      loop_control:
        label: "{{ item.0.metadata.name }} / {{ item.1.name }}"
      register: init_logs
      ignore_errors: yes

    - name: Display init container logs
      debug:
        var: init_logs.results
  when: kyverno_status is failed

- name: Copy Kyverno policy to force Open5GS pods to {{ groups['core_node'][0] }}
  copy:
    dest: /tmp/force-open5gs-node.yaml
    content: |
      apiVersion: kyverno.io/v1
      kind: ClusterPolicy
      metadata:
        name: force-open5gs-to-core-node
      spec:
        mutateExistingOnPolicyUpdate: true
        rules:
          - name: add-open5gs-node-selector
            match:
              resources:
                kinds: ["Pod"]
                selector:
                  matchLabels:
                    app: open5gs
            mutate:
              patchStrategicMerge:
                spec:
                  nodeSelector:
                    open5gs-core-node: "true"

- name: Apply Kyverno policy
  command: kubectl apply -f /tmp/force-open5gs-node.yaml
  changed_when: false

- name: Clone open5gs-k8s repo
  ansible.builtin.git:
    repo: "{{ repo_url }}"
    dest: "{{ repo_dest }}"
    version: main
    update: yes
    force: yes

- name: Ensure {{ open5gs_ns }} namespace exists
  kubernetes.core.k8s:
    api_version: v1
    kind: Namespace
    name: "{{ open5gs_ns }}"
    state: present

- name: Apply MongoDB with Kustomize
  ansible.builtin.command: kubectl apply -k "{{ repo_dest }}/mongodb" -n {{ open5gs_ns }}

- name: Wait for MongoDB pod to be Ready
  kubernetes.core.k8s_info:
    kind: Pod
    namespace: "{{ open5gs_ns }}"
    label_selectors:
      - app.kubernetes.io/name=mongodb
  register: mongo_pods
  until: >
    mongo_pods.resources | length > 0 and
    (mongo_pods.resources[0].status.containerStatuses is defined) and
    (mongo_pods.resources[0].status.containerStatuses | length > 0) and
    (mongo_pods.resources[0].status.containerStatuses[0].ready | default(false))
  retries: 40
  delay: 5

# === NETWORK ATTACHMENT DEFINITIONS (NADs) ===
- name: Apply NADs with Kustomize
  ansible.builtin.command: kubectl apply -k "{{ repo_dest }}/networks5g" -n {{ open5gs_ns }}

- name: Wait for kube-apiserver to be responsive
  shell: kubectl get --raw='/healthz'
  register: api_health
  retries: 30
  delay: 5
  until: api_health.rc == 0
  changed_when: false

- name: Wait for CRDs registration to be ready
  shell: kubectl get crds
  register: crd_list
  retries: 20
  delay: 5
  until: "'network-attachment-definitions.k8s.cni.cncf.io' in crd_list.stdout"
  changed_when: false

- name: Install jq (required for NAD patching)
  ansible.builtin.apt:
    name: jq
    state: present
    update_cache: yes
  become: yes

- name: Wait for Multus CRD to be registered
  ansible.builtin.shell: kubectl get crd network-attachment-definitions.k8s.cni.cncf.io
  register: multus_crd_check
  retries: 60
  delay: 5
  until: multus_crd_check.rc == 0
  changed_when: false
  failed_when: false

- name: Patch NADs to use unique interface names (net1/net2/net3)
  shell: |
    set -euo pipefail
    for nad in n2network n3network n4network; do
      case "$nad" in
        n2network) ifname=net1 ;;
        n3network) ifname=net2 ;;
        n4network) ifname=net3 ;;
        *) continue ;;
      esac
      if ! kubectl get networkattachmentdefinition "$nad" -n open5gs &>/dev/null; then
        echo "Skipping $nad (not yet created)"
        continue
      fi
      config=$(kubectl get networkattachmentdefinition "$nad" -n open5gs -o jsonpath='{.spec.config}')
      newconfig=$(echo "$config" | jq --arg n "$ifname" '.ifname = $n')
      kubectl patch networkattachmentdefinition "$nad" -n open5gs --type=merge -p "{\"spec\":{\"config\":$newconfig}}"
    done
  register: patch_nad
  failed_when: false
  changed_when: "'Skipping' not in patch_nad.stdout"

- name: Verify required NADs exist
  kubernetes.core.k8s_info:
    kind: NetworkAttachmentDefinition
    namespace: "{{ open5gs_ns }}"
  register: nad_info

- name: Fail if a required NAD is missing
  ansible.builtin.fail:
    msg: "NAD {{ item }} is missing in {{ open5gs_ns }}"
  when: item not in (nad_info.resources | map(attribute='metadata.name') | list)
  loop: "{{ nad_list }}"

- name: Apply Open5GS NFs with Kustomize
  ansible.builtin.command: kubectl apply -k "{{ repo_dest }}/{{ deployment_option }}" -n {{ open5gs_ns }}

- name: Wait for Open5GS Core NFs pods Ready
  kubernetes.core.k8s_info:
    kind: Pod
    namespace: "{{ open5gs_ns }}"
    label_selectors:
      - "nf={{ item }}"
  register: nf_pod
  until: >
    nf_pod.resources | length > 0 and
    (nf_pod.resources[0].status.containerStatuses is defined) and
    (nf_pod.resources[0].status.containerStatuses | length > 0) and
    (nf_pod.resources[0].status.containerStatuses[0].ready | default(false))
  retries: 40
  delay: 5
  loop: "{{ nfs }}"
  loop_control:
    label: "{{ item }}"
- name: Install python3-venv (required for venv)
  ansible.builtin.apt:
    name: python3-venv
    state: present
  become: yes

- name: Create Python virtual environment for Open5GS admin scripts
  ansible.builtin.command:
    cmd: python3 -m venv "{{ repo_dest }}/venv"
    creates: "{{ repo_dest }}/venv/bin/activate"
  become: yes

- name: Upgrade pip in virtualenv
  ansible.builtin.pip:
    name: pip
    state: latest
    executable: "{{ repo_dest }}/venv/bin/pip"
  become: yes

- name: Install Python requirements for admin script
  ansible.builtin.pip:
    requirements: "{{ repo_dest }}/requirements.txt"
    executable: "{{ repo_dest }}/venv/bin/pip"
  become: yes

- name: Deploy Open5GS Web UI with Kustomize
  ansible.builtin.command: kubectl apply -k "{{ repo_dest }}/open5gs-webui" -n {{ open5gs_ns }}

- name: Wait for WebUI pod Ready
  kubernetes.core.k8s_info:
    kind: Pod
    namespace: "{{ open5gs_ns }}"
    label_selectors:
      - "nf=webui"
  register: webui_pod
  until: >
    webui_pod.resources | length > 0 and
    (webui_pod.resources[0].status.containerStatuses is defined) and
    (webui_pod.resources[0].status.containerStatuses | length > 0) and
    (webui_pod.resources[0].status.containerStatuses[0].ready | default(false))
  retries: 40
  delay: 5

- name: Run add-admin-account.py
  ansible.builtin.command: >
    {{ repo_dest }}/venv/bin/python mongo-tools/add-admin-account.py
  args:
    chdir: "{{ repo_dest }}"

- name: Add subscribers
  ansible.builtin.shell: |
    {{ repo_dest }}/venv/bin/python mongo-tools/generate-data.py && \
    {{ repo_dest }}/venv/bin/python mongo-tools/add-subscribers.py
  args:
    chdir: "{{ repo_dest }}"
  register: sub_out
  changed_when: false

- name: Check added subscribers
  ansible.builtin.shell: |
    {{ repo_dest }}/venv/bin/python mongo-tools/check-subscribers.py
  args:
    chdir: "{{ repo_dest }}"
  register: check_out
  changed_when: false

- name: Show Added subscribers
  debug:
    var: check_out.stdout