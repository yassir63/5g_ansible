---

- name: RESTART_K8S_CREATE
  ansible.builtin.meta: noop

# Safety cleanup
- name: Reset Kubernetes if a previous init was attempted
  ansible.builtin.command: kubeadm reset -f
  ignore_errors: yes
  become: true

- name: Remove old Kubernetes data directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes/pki
    - /etc/kubernetes/manifests
    - /var/lib/etcd
    - /var/lib/kubelet
  become: true

# Upload config
- name: Upload kubeadm config to master node
  ansible.builtin.copy:
    src: kubeadm-config.yaml
    dest: /root/kubeadm-config.yaml
    mode: '0644'
  become: true

# Fix kubeadm config (v1beta3 â†’ v1beta4)
- name: Migrate kubeadm config to v1beta4
  ansible.builtin.shell: |
    kubeadm config migrate --old-config /root/kubeadm-config.yaml --new-config /root/kubeadm-config-new.yaml
    mv /root/kubeadm-config-new.yaml /root/kubeadm-config.yaml
  become: true
  when: inventory_hostname == groups['core_node'][0]

- name: Configure containerd to use overlayfs
  ansible.builtin.copy:
    dest: /etc/containerd/config.toml
    content: |
      version = 2
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        SystemdCgroup = true
      [plugins."io.containerd.snapshotter.v1.overlayfs"]
        snapshotter = "overlayfs"
    mode: '0644'
  become: true
  when: inventory_hostname == groups['core_node'][0]

- name: Restart containerd BEFORE image pull
  ansible.builtin.systemd:
    name: containerd
    state: restarted
  become: true
  when: inventory_hostname == groups['core_node'][0]

# Pre-pull images
- name: Pre-pull Kubernetes images
  ansible.builtin.shell: kubeadm config images pull --config /root/kubeadm-config.yaml
  become: true
  when: inventory_hostname == groups['core_node'][0]
  ignore_errors: yes

# Initialize cluster
- name: Initialize Kubernetes cluster with kubeadm config
  ansible.builtin.command: kubeadm init --config /root/kubeadm-config.yaml
  register: kubeadm_init
  failed_when: kubeadm_init.rc != 0
  changed_when: kubeadm_init.rc == 0
  become: true
  when: inventory_hostname == groups['core_node'][0]

# Setup kubeconfig
- name: Create .kube directory in user's home
  ansible.builtin.file:
    path: "{{ ansible_env.HOME }}/.kube"
    state: directory
    mode: '0700'
  when: inventory_hostname == groups['core_node'][0]

- name: Copy admin.conf to kube config
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ ansible_env.HOME }}/.kube/config"
    remote_src: yes
    mode: '0600'
  become: true
  when: inventory_hostname == groups['core_node'][0]

# Install Helm + Kubernetes Python client (PEP-668 safe)
- name: Install python3-venv and python3-pip
  ansible.builtin.apt:
    name:
      - python3-venv
      - python3-pip
      - curl
    state: present
    update_cache: yes
  become: true

- name: Create Helm/K8s venv
  ansible.builtin.command:
    cmd: python3 -m venv /opt/k8s-venv
    creates: /opt/k8s-venv/bin/activate
  become: true

- name: Install kubernetes Python client in venv
  ansible.builtin.pip:
    name: kubernetes
    state: present
    executable: /opt/k8s-venv/bin/pip
  become: true

- name: Set ansible_python_interpreter for k8s modules
  ansible.builtin.set_fact:
    ansible_python_interpreter: /opt/k8s-venv/bin/python
  when: inventory_hostname == groups['core_node'][0]

# Install Helm
- name: Download Helm install script
  ansible.builtin.get_url:
    url: https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    dest: /tmp/get-helm-3
    mode: '0755'
  become: true

- name: Install Helm
  ansible.builtin.command: /tmp/get-helm-3
  args:
    creates: /usr/local/bin/helm
  become: true

# Install Flannel
- name: Create kube-flannel namespace with pod-security label
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: kube-flannel
        labels:
          pod-security.kubernetes.io/enforce: privileged
    state: present
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

- name: Add Flannel Helm repo
  kubernetes.core.helm_repository:
    name: flannel
    repo_url: https://flannel-io.github.io/flannel/
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

- name: Install Flannel via Helm
  kubernetes.core.helm:
    name: flannel
    chart_ref: flannel/flannel
    release_namespace: kube-flannel
    state: present
    wait: true
    wait_timeout: 300s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

# Install Multus
- name: Upload Multus DaemonSet manifest
  ansible.builtin.copy:
    src: multus-daemonset.yml
    dest: /root/multus-daemonset.yml
    mode: '0644'
  become: true
  when: inventory_hostname == groups['core_node'][0]

- name: Apply Multus DaemonSet
  kubernetes.core.k8s:
    state: present
    src: /root/multus-daemonset.yml
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

- name: Wait for Multus DaemonSet to be ready
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: DaemonSet
    namespace: kube-system
    name: kube-multus-ds
  register: multus_ds_info
  until:
    - multus_ds_info.resources | length > 0
    - multus_ds_info.resources[0].status.desiredNumberScheduled == multus_ds_info.resources[0].status.numberReady
  retries: 40
  delay: 15
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

# Wait for CoreDNS
- name: Wait for CoreDNS pods to be Ready
  ansible.builtin.shell: kubectl wait -n kube-system --for=condition=Ready pod -l k8s-app=kube-dns --timeout=300s
  register: coredns_wait
  retries: 5
  delay: 10
  until: coredns_wait.rc == 0
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

# Remove taint
- name: Remove NoSchedule taint from control-plane node
  ansible.builtin.command: kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
  failed_when: false
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == groups['core_node'][0]

# Generate join command
- name: Generate kubeadm join command
  ansible.builtin.shell: kubeadm token create --print-join-command
  register: join_command
  run_once: true
  delegate_to: "{{ groups['core_node'][0] }}"

- name: Save join command to file
  ansible.builtin.copy:
    content: "{{ join_command.stdout }}"
    dest: /tmp/kubeadm_join_command.txt
    mode: '0644'
  run_once: true
  delegate_to: localhost
  