---
- name: RESTART_K8S_CREATE
  ansible.builtin.meta: noop

# === CLEANUP PHASE ============================================================
- name: Reset Kubernetes (safe)
  ansible.builtin.command: kubeadm reset -f
  ignore_errors: true
  become: true

- name: Remove old Kubernetes and CNI directories (safe)
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes
    - /var/lib/etcd
    - /var/lib/kubelet
    - /etc/cni
    - /opt/cni
    - /var/lib/cni
    - /run/flannel
  become: true
  ignore_errors: true

# Expand /var/lib/containerd to tmpfs (only if not already done) ===
# === OPTIONAL: Expand /var/lib/containerd to tmpfs (only if not already done) ===
- name: Ensure /var/lib/containerd is mounted on tmpfs (>=50G)
  ansible.builtin.shell: |
    current_mount=$(mountpoint -q /var/lib/containerd && df -h --output=fstype,size /var/lib/containerd | tail -n1 || echo "none 0G")
    fstype=$(echo "$current_mount" | awk '{print $1}')
    size=$(echo "$current_mount" | awk '{print $2}' | sed 's/G//')
    if [ "$fstype" != "tmpfs" ] || [ "${size:-0}" -lt 50 ]; then
      echo "Mounting tmpfs on /var/lib/containerd (50G)"
      umount -f /var/lib/containerd 2>/dev/null || true
      mount -t tmpfs -o size=50G tmpfs /var/lib/containerd
    else
      echo "tmpfs already mounted on /var/lib/containerd with ${size}G"
    fi
  become: true
  ignore_errors: true
  register: containerd_mount

- name: Restart containerd after remount
  ansible.builtin.systemd:
    name: containerd
    state: restarted
  become: true

- name: Wait 5 seconds for containerd to settle
  ansible.builtin.pause:
    seconds: 5

# === KUBEADM INIT =============================================================
- name: Upload kubeadm config to master node
  ansible.builtin.copy:
    src: kubeadm-config.yaml
    dest: /root/kubeadm-config.yaml
    mode: '0644'
  become: true

- name: Initialize Kubernetes cluster
  ansible.builtin.command: kubeadm init --config /root/kubeadm-config.yaml
  register: kubeadm_init
  become: true

# ---- Sanitize kubelet extra args: remove deprecated CNI flags if present ----
- name: Check for deprecated kubelet flags in /etc/default/kubelet
  ansible.builtin.shell: |
    if [ -f /etc/default/kubelet ]; then
      if grep -Eq -- '--network-plugin|--cni-bin-dir|--cni-conf-dir' /etc/default/kubelet; then
        echo "DEPRECATED_FOUND"
      else
        echo "CLEAN"
      fi
    else
      echo "MISSING"
    fi
  register: kubelet_extra_check
  changed_when: false
  become: true

- name: Backup existing /etc/default/kubelet if deprecated flags found
  ansible.builtin.copy:
    src: /etc/default/kubelet
    dest: /etc/default/kubelet.bak_{{ ansible_date_time.epoch }}
    remote_src: yes
  when: kubelet_extra_check.stdout == "DEPRECATED_FOUND"
  become: true

- name: Sanitize /etc/default/kubelet (write only the resolvConf) if deprecated flags found or file missing
  ansible.builtin.copy:
    dest: /etc/default/kubelet
    content: 'KUBELET_EXTRA_ARGS="--resolv-conf=/run/systemd/resolve/resolv.conf"'
    mode: '0644'
  when: kubelet_extra_check.stdout != "CLEAN"
  become: true

- name: Ensure kubelet systemd sees the change (daemon-reload)
  ansible.builtin.systemd:
    daemon_reload: yes
  become: true

- name: Restart kubelet if we changed the file
  ansible.builtin.systemd:
    name: kubelet
    state: restarted
  when: kubelet_extra_check.stdout != "CLEAN"
  become: true

# === FIX: Force kubelet to use correct resolv.conf ============================
- name: Ensure kubelet uses systemd-resolved DNS
  ansible.builtin.copy:
    dest: /etc/default/kubelet
    content: |
      KUBELET_EXTRA_ARGS=--resolv-conf=/run/systemd/resolve/resolv.conf
    mode: "0644"
  become: true


- name: Restart kubelet to apply DNS config
  ansible.builtin.systemd:
    name: kubelet
    state: restarted
  become: true

- name: Enable cluster CIDR allocation in kube-controller-manager
  ansible.builtin.lineinfile:
    path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    regexp: "{{ item.regexp }}"
    line: "{{ item.line }}"
  loop:
    - { regexp: '^\s*- --allocate-node-cidrs=', line: '    - --allocate-node-cidrs=true' }
    - { regexp: '^\s*- --cluster-cidr=', line: '    - --cluster-cidr=10.244.0.0/16' }
  become: true

- name: Create .kube directory in user's home
  file:
    path: "{{ ansible_env.HOME }}/.kube"
    state: directory
    mode: '0700'

- name: Copy admin.conf to kube config
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ ansible_env.HOME }}/.kube/config"
    remote_src: yes
    mode: '0600'
  become: true

# === TOOLING ==================================================================
- name: Ensure Helm and Python tooling are available
  ansible.builtin.apt:
    name:
      - python3-pip
      - curl
    state: present
    update_cache: yes
  become: true

- name: Install kubernetes Python module
  ansible.builtin.pip:
    name: kubernetes
  become: true

- name: Install Helm if missing
  ansible.builtin.shell: |
    curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  args:
    creates: /usr/local/bin/helm
  become: true

# === FLANNEL (CNI) ============================================================
- name: Ensure CNI directories exist
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: '0755'
  loop:
    - /etc/cni/net.d
    - /opt/cni/bin
    - /var/lib/cni
  become: true

# --- CLEAN old/conflicting CNI config files (Multus/old Flannel) -------------
- name: Clean old or conflicting CNI configs
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/cni/net.d/00-multus.conf
    - /etc/cni/net.d/00-multus.conf.bak
    - /etc/cni/net.d/10-flannel.conf
    - /etc/cni/net.d/10-flannel.conf~
  ignore_errors: yes
  become: true

- name: Deploy Flannel CNI conflist
  ansible.builtin.copy:
    dest: /etc/cni/net.d/10-flannel.conflist
    mode: '0644'
    content: |
      {
        "name": "cbr0",
        "cniVersion": "0.3.1",
        "plugins": [
          {
            "type": "flannel",
            "delegate": {
              "hairpinMode": true,
              "isDefaultGateway": true
            }
          },
          {
            "type": "portmap",
            "capabilities": {
              "portMappings": true
            }
          }
        ]
      }
  become: true

- name: Download and install CNI plugins
  ansible.builtin.unarchive:
    src: "https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz"
    dest: /opt/cni/bin
    remote_src: true
  become: true

- name: Ensure kube-flannel namespace exists
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: kube-flannel
        labels:
          pod-security.kubernetes.io/enforce: privileged
    state: present
    kubeconfig: /etc/kubernetes/admin.conf

- name: Add Flannel Helm repo
  kubernetes.core.helm_repository:
    name: flannel
    repo_url: https://flannel-io.github.io/flannel/
    kubeconfig: /etc/kubernetes/admin.conf

- name: Install Flannel via Helm
  kubernetes.core.helm:
    name: flannel
    chart_ref: flannel/flannel
    release_namespace: kube-flannel
    state: present
    wait: true
    kubeconfig: /etc/kubernetes/admin.conf
    values:
      podCidr: "10.244.0.0/16"
      flannel:
        iface: "{{ hostvars[inventory_hostname].nic_interface | default('eth0') }}"

# === FIX CNI INITIALIZATION ===================================================
- name: Restart containerd and kubelet to reload CNI
  ansible.builtin.systemd:
    name: "{{ item }}"
    state: restarted
  loop:
    - containerd
    - kubelet
  become: true

- name: Wait until node is Ready
  ansible.builtin.shell: |
    kubectl wait node $(hostname) --for=condition=Ready --timeout=180s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: wait_ready
  retries: 3
  delay: 10
  until: wait_ready.rc == 0
  become: true

# === COREDNS & FINALIZATION ===================================================
- name: Wait for CoreDNS pods to be Ready
  ansible.builtin.shell: |
    kubectl wait -n kube-system --for=condition=Ready pod -l k8s-app=kube-dns --timeout=300s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  become: true

- name: Test CoreDNS resolution using a non-interactive busybox pod
  become: true
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  ansible.builtin.shell: |
    kubectl run dns-test --image=busybox:1.36 --restart=Never --command -- \
      sh -c 'nslookup kubernetes.default.svc.cluster.local' || true
        echo "Cleaning up test pods..."
    kubectl delete pod dns-test --ignore-not-found
  changed_when: false

- name: Remove control-plane taint
  ansible.builtin.shell: |
    kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  become: true
  when: full_oai == false

# === JOIN COMMAND =============================================================
- name: Generate join command
  ansible.builtin.shell: kubeadm token create --print-join-command
  register: join_cmd
  delegate_to: "{{ groups['core_node'][0] }}"
  run_once: true

- name: Save join command locally
  ansible.builtin.copy:
    content: "{{ join_cmd.stdout }}"
    dest: /tmp/kubeadm_join_command.txt
    mode: '0644'
  delegate_to: localhost
  run_once: true
