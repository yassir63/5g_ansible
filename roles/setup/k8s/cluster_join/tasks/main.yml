---
- name: Check if node is already part of a Kubernetes cluster
  stat:
    path: /etc/kubernetes/kubelet.conf
  register: kubelet_conf

- name: Reset node if already part of a cluster
  command: kubeadm reset -f
  when: kubelet_conf.stat.exists
  ignore_errors: true

- name: Remove Kubernetes CNI configuration
  file:
    path: /etc/cni/net.d
    state: absent
  when: kubelet_conf.stat.exists

- name: Remove kubelet data directory
  file:
    path: /var/lib/kubelet
    state: absent
  when: kubelet_conf.stat.exists

- name: Remove Kubernetes manifest directory
  file:
    path: /etc/kubernetes/manifests
    state: absent
  when: kubelet_conf.stat.exists

- name: Restart container runtime (containerd)
  service:
    name: containerd
    state: restarted
  when: kubelet_conf.stat.exists
  ignore_errors: true


- name: Read kubeadm join command from file
  set_fact:
    kubeadm_join_command: "{{ lookup('file', '/tmp/kubeadm_join_command.txt') }}"

- name: Debug join command
  debug:
    var: kubeadm_join_command

- name: Join the node to the cluster
  command: "{{ kubeadm_join_command }} --ignore-preflight-errors=SystemVerification --v=5"

- name: Verify reservedSystemCPUs in kubelet config on the worker
  become: true
  command: grep 'reservedSystemCPUs' /var/lib/kubelet/config.yaml
  register: kubelet_reserved
  failed_when: "'0-5' not in kubelet_reserved.stdout"
  changed_when: false
  when: hostvars[groups['faraday'][0]].rru in ['benetel1', 'benetel2']

- name: Show verification result
  debug:
    msg: "reservedSystemCPUs is correctly set to 0-5 in kubelet config"
  when: hostvars[groups['faraday'][0]].rru in ['benetel1', 'benetel2']

- name: Create .kube directory on worker node
  file:
    path: /root/.kube
    state: directory
    mode: '0755'

- name: Fetch admin.conf from master to control machine
  fetch:
    src: /etc/kubernetes/admin.conf
    dest: ./admin.conf
    flat: yes
  delegate_to: "{{ groups['core_node'][0] }}"
  when: inventory_hostname == groups['k8s_workers'][0] # avoid checksum conflict error

- name: Copy admin.conf from control machine to worker node
  copy:
    src: ./admin.conf
    dest: /root/.kube/config
    mode: '0644'
    force: yes
  become: true

- name: Wait for kubelet to start and become active
  systemd:
    name: kubelet
    state: started
    enabled: yes
